{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](DataKind_orange.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NRGI Extractives Contracts\n",
    "## Featurize and Predict\n",
    "### 1. Reads in text (already parsed by paragraph and HTML stripped)\n",
    "### 2. Cleans text\n",
    "### 3. Featurizes\n",
    "### 4. Predicts using previously pickled model\n",
    "### 5. Outputs results to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, io\n",
    "from collections import Counter\n",
    "from itertools import product\n",
    "import re, string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from textblob import TextBlob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook', rc={'figure.figsize': (10, 6)}, font_scale=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_input_file = 'contract_data/cleaned_unannotated_contracts_by_paragraph_en.pkl'\n",
    "pickled_model = 'models/model_3_classes.pkl'\n",
    "pickled_tfidf = 'models/tfidf_vectorizer.pkl'\n",
    "output_results = 'results_classification/unannotated_corpus_predictions_3_classes_en.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spacy is used for Part of Speech tagging and Named Entity Recognition\n",
    "# spacy is a non-standard python library which can be installed using 'pip install spacy' from the command line\n",
    "# language models can be downloaded by running 'python -m spacy download <language>' from the command line\n",
    "import spacy\n",
    "supported_languages = ['en','fr','es']\n",
    "language_dict = {'en':'english','fr':'french','es':'spanish'}\n",
    "nlp_langs = {}\n",
    "for language in supported_languages:\n",
    "    nlp_langs[language]  = spacy.load(language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# contracts_by_para = pd.read_pickle(data_input_file)\n",
    "model = joblib.load(pickled_model)\n",
    "tfidf_vectorizer = joblib.load(pickled_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(contracts_by_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def punctuation_remove(text):\n",
    "    \"\"\"\n",
    "    Mutates and returns text where all punctuation besides underscores,\n",
    "    are replaced\n",
    "    \"\"\"\n",
    "    punctuation_re = r'[^\\w\\s_]'\n",
    "    new_text = nltk.regexp.re.sub(punctuation_re, ' ', text)\n",
    "    return new_text\n",
    "\n",
    "def replace_numbers(text):\n",
    "    ''' \n",
    "    Removes all characters but periods, commas and alpha-numeric and \n",
    "    returns all numeric values replace with the word numeric_value\n",
    "    '''\n",
    "#     allowed = {\",\", \".\",\" \",\"%\"}.union(string.ascii_letters).union([str(num) for num in range(0,10)])\n",
    "#     filtered = ''.join([character for character in text if character in allowed])\n",
    "    wordlist = text.split()\n",
    "    for i in range(len(wordlist)):\n",
    "        if '$' in wordlist[i]:\n",
    "            try:\n",
    "                int(wordlist[i].split('$')[-1].replace(',','').replace('.','').replace('-','').replace(')','').replace('(','').replace('\\'','').replace(';','').replace(':',''))\n",
    "                wordlist[i] = ' '.join(wordlist[i].split('$')[:-1]) + ' dollarvalue'\n",
    "            except:\n",
    "                pass\n",
    "        elif '%' in wordlist[i]:\n",
    "            try: \n",
    "                int(wordlist[i].replace(',','').replace('.','').replace('-','').replace('/','').replace('%','').replace(')','').replace('(','').replace('\\'','').replace(';','').replace(':',''))\n",
    "                wordlist[i] = 'percentvalue'\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            try: \n",
    "                int(wordlist[i].replace(',','').replace('.','').replace('-','').replace('/','').replace(')','').replace('(','').replace('\\'','').replace(';','').replace(':',''))\n",
    "                wordlist[i] = 'numericvalue'\n",
    "            except:\n",
    "                pass\n",
    "    return ' '.join(wordlist)\n",
    "\n",
    "def perform_lowercase(text):\n",
    "    \"\"\"\n",
    "    Mutates and returns text where all characters are lowercased\n",
    "    \"\"\"\n",
    "    try:\n",
    "        new_text = text.lower()\n",
    "    except:\n",
    "        new_text = str(text).lower()\n",
    "    return new_text\n",
    "\n",
    "def underscore_remove(text):\n",
    "    '''\n",
    "    replaces multiple underscores with text fillintheblank\n",
    "    and single underscore with space\n",
    "    '''\n",
    "    double_underscore_re = r'(__[a-zA-Z0-9_]*(__)?)'\n",
    "    text = nltk.regexp.re.sub(double_underscore_re,'fillintheblank',text)\n",
    "    return text.replace('_',' ')\n",
    "\n",
    "def doublespace_remove(text):\n",
    "    return re.sub(' +',' ',text)\n",
    "\n",
    "def textblobsent(text):\n",
    "    '''\n",
    "    returns the TextBlob polarity and subjectivity\n",
    "    '''\n",
    "    text = text.encode('ascii','replace')\n",
    "    sent = TextBlob(text).sentiment\n",
    "    return pd.Series([sent.polarity,sent.subjectivity])\n",
    "\n",
    "def get_avg_wordlength(document):\n",
    "    wordlengths = [len(word) for word in document.split()]\n",
    "    if len(wordlengths) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.mean(wordlengths)\n",
    "\n",
    "def get_multilingual_pos(text,language):\n",
    "    if language in nlp_langs.keys():\n",
    "        try:\n",
    "            tokens = nlp_langs[language](text)\n",
    "            tags = [token.pos_ for token in tokens]\n",
    "            counts = Counter(tags).items()\n",
    "            countdict = {}\n",
    "            for key, value in counts:\n",
    "                countdict[key] = value\n",
    "            return countdict\n",
    "        except:\n",
    "            return {'pos_error':1}\n",
    "    else:\n",
    "        return {'unsupported_language':1}\n",
    "\n",
    "def get_multilingual_entities(row):\n",
    "    \n",
    "    text = row['CleanText']\n",
    "    language = row['Language']\n",
    "    if language in nlp_langs.keys():\n",
    "        try:\n",
    "            doc = nlp_langs[language](text)\n",
    "            labels = [ent.label_ for ent in doc.ents]\n",
    "            texts = [ent.text for ent in doc.ents]\n",
    "            starts = [ent.start_char for ent in doc.ents]\n",
    "            ends = [ent.end_char for ent in doc.ents]\n",
    "            textlens = [len(word) for word in texts]\n",
    "            labellens = [len(label) for label in labels]\n",
    "            diffs = [textlens[i] - labellens[i] for i in range(len(textlens))]\n",
    "            diffsum = [sum(diffs[0:i]) for i in range(len(diffs))]\n",
    "\n",
    "            for i in range(len(labels)):\n",
    "                text = text[0:starts[i] - diffsum[i]] + 'entity' + labels[i] + text[ends[i] - diffsum[i]:]\n",
    "            return text\n",
    "        except:\n",
    "            return text\n",
    "    else:\n",
    "        return text\n",
    "    \n",
    "def remove_stopwords(row):\n",
    "    '''\n",
    "    Multilingual stopwords removal\n",
    "    '''\n",
    "    try:\n",
    "        language = language_dict[row['Language']]\n",
    "        text = ' '.join([word for word in row['CleanText'].split(' ') if word not in stopwords.words(language)])\n",
    "        return text\n",
    "    except:\n",
    "        return row['CleanText']\n",
    "    \n",
    "def stem_words(row):\n",
    "    ''' \n",
    "    Multilingual word stemmer\n",
    "    '''\n",
    "    language = language_dict[row['Language']]\n",
    "    try:\n",
    "        stemmer = SnowballStemmer(language)\n",
    "        text = ' '.join([stemmer.stem(word) for word in row['CleanText_NoStop'].split(' ')])\n",
    "        return text\n",
    "    except:\n",
    "        return row['CleanText_NoStop']\n",
    "    \n",
    "def clean_metadata(text):\n",
    "    if type(text) in [float,int]:\n",
    "        return text\n",
    "    elif type(text) == str:\n",
    "        return text.lower().split(';')[0]\n",
    "    else:\n",
    "        text = text.encode('ascii','replace')\n",
    "        return text.lower().split(';')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(df):\n",
    "    df['Clean_Paragraph_Text'].fillna('',inplace=True)\n",
    "    df['CleanText'] = df['Clean_Paragraph_Text']\n",
    "    df['CleanText'] = df.apply(get_multilingual_entities,axis=1)\n",
    "    func_list = [perform_lowercase,replace_numbers,punctuation_remove,underscore_remove, doublespace_remove]\n",
    "    for func in func_list:\n",
    "        df['CleanText'] = df['CleanText'].apply(func)\n",
    "        print func\n",
    "    return df\n",
    "\n",
    "def featurize(df):\n",
    "    \n",
    "    df['AvgWordLength'] = df['CleanText'].apply(get_avg_wordlength)\n",
    "    print 'AvgWordLength complete'\n",
    "    df['CleanText_NoStop'] = df.apply(remove_stopwords,axis=1)\n",
    "    df['CleanText_NoStop_Stemmed'] = df.apply(stem_words,axis=1)\n",
    "    print 'stemming complete'\n",
    "    postagcounts = []\n",
    "    entitycounts = []\n",
    "    for index, row in df.iterrows():\n",
    "        postagcounts.append(get_multilingual_pos(row['Clean_Paragraph_Text'],row['Language']))    \n",
    "    postagdf = pd.DataFrame(postagcounts).fillna(0)\n",
    "    postagdf.index = df.index\n",
    "    postagdf.columns = ['postag_' + col for col in postagdf.columns]\n",
    "    print 'postagging complete'\n",
    "    \n",
    "    # create dummy variables for categoricals\n",
    "    df['Resource'] = df['Resource'].apply(clean_metadata)\n",
    "    df['Contract Type'] = df['Contract Type'].apply(clean_metadata)\n",
    "    df['Document Type'] = df['Document Type'].apply(lambda x: x.lower().split(';')[0])\n",
    "    dummy_cols = ['Language','Country Name','Resource','Contract Type','Document Type']\n",
    "    dummies = pd.get_dummies(df[dummy_cols],prefix = dummy_cols)\n",
    "    print 'dummies complete'\n",
    "    # drop lowest least frequent dummy columns for each\n",
    "    for dummy_col in dummy_cols:\n",
    "        cols = [col for col in dummies.columns if col.startswith(dummy_col)]\n",
    "        dummies.drop([col for col, val in dummies[cols].sum().iteritems() if val == dummies[cols].sum().min()],axis=1,inplace=True)\n",
    "    df.drop(dummy_cols, axis=1,inplace=True)\n",
    "        \n",
    "    textblobsentdf = df['CleanText'].apply(textblobsent)\n",
    "    textblobsentdf.columns = ['TextblobPolarity','TextblobSubjectivity']\n",
    "    print 'textblob complete'\n",
    "    df = pd.concat([df,textblobsentdf,postagdf,dummies],axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "contracts_by_para = clean_text(contracts_by_para)\n",
    "contracts_by_para = featurize(contracts_by_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# contracts_by_para.to_pickle('contracts_by_para_featurized.pkl')\n",
    "contracts_by_para = pd.read_pickle('contracts_by_para_featurized.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192\n",
      "164\n"
     ]
    }
   ],
   "source": [
    "exclude = ['Source','Category','Topic','Annotation Text','CleanText','CleanText_NoStop','CleanText_NoStop_Stemmed',\n",
    "           'OCID','PDF Page Number','Article Reference','MD','VBP','VBZ','VBG','VBD','VBN','other',\"''\",'label','sort_key','Corrected']\n",
    "features = [str(col) for col in contracts_by_para.columns.tolist() if not col in exclude]\n",
    "print len(features)\n",
    "fitted_features = pickle.load(open('models/fitted_feature_list.pkl','rb'))\n",
    "print len(fitted_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166\n",
      "165\n",
      "164\n"
     ]
    }
   ],
   "source": [
    "for col in fitted_features:\n",
    "    if not col in features:\n",
    "        print col\n",
    "        contracts_by_para[col] = 0\n",
    "        features.append(col)\n",
    "print len(features)\n",
    "for col in features:\n",
    "    if not col in fitted_features:\n",
    "        features.remove(col)\n",
    "print len(features)\n",
    "for col in features:\n",
    "    if not col in fitted_features:\n",
    "        features.remove(col)\n",
    "print len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 30000, 60000, 90000, 120000, 150000, 180000, 210000, 240000, 252816]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = [i for i in xrange(0,len(contracts_by_para),30000)]\n",
    "chunks.append(len(contracts_by_para))\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(chunks)-1):\n",
    "    df = contracts_by_para[chunks[i]:chunks[i+1]].copy()\n",
    "    df.to_pickle('contract_data/en_featurized_chunk_' + str(i) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del contracts_by_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(30000, 8449)\n",
      "predicting\n",
      "1\n",
      "(30000, 8449)\n",
      "predicting\n",
      "2\n",
      "(30000, 8449)\n",
      "predicting\n",
      "3\n",
      "(30000, 8449)\n",
      "predicting\n",
      "4\n",
      "(30000, 8449)\n",
      "predicting\n",
      "5\n",
      "(30000, 8449)\n",
      "predicting\n",
      "6\n",
      "(30000, 8449)\n",
      "predicting\n",
      "7\n",
      "(30000, 8449)\n",
      "predicting\n",
      "8\n",
      "(12816, 8449)\n",
      "predicting\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(chunks)-1):\n",
    "    print i\n",
    "    df = pd.read_pickle('contract_data/en_featurized_chunk_' + str(i) + '.pkl')\n",
    "    output_results = 'results_classification/unannotated_corpus_predictions_3_classes_en_' + str(i) + '.pkl'\n",
    "    tfidf_matrix = tfidf_vectorizer.transform(df['CleanText_NoStop_Stemmed'].values.astype('U'))\n",
    "    terms = tfidf_vectorizer.get_feature_names()\n",
    "    tfidf_matrix = tfidf_matrix.todense()\n",
    "    tfidf = pd.DataFrame(tfidf_matrix)\n",
    "    tfidf.index = df.index\n",
    "    tfidf.columns = terms\n",
    "    print tfidf.shape\n",
    "    X = pd.concat([tfidf,df[features]],axis=1)\n",
    "    X.fillna(0,inplace=True)\n",
    "    X = X.rename(columns = {'fit':'fit_feature'})\n",
    "    print 'predicting'\n",
    "    predictions = model.predict(X)\n",
    "    df['Predicted_Clause'] = predictions\n",
    "    df[['OCID','Paragraph_Num','Source','Clean_Paragraph_Text','CleanText','Predicted_Clause']].to_pickle(output_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
