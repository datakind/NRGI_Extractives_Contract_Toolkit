{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](DataKind_orange.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performs grid search for binary classification of Stabilization and Royalties clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, io\n",
    "from collections import Counter\n",
    "from itertools import product\n",
    "import re, string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# from sklearn.dummy import DummyClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# , ExtraTreesClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, recall_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from textblob import TextBlob\n",
    "from textstat.textstat import textstat\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook', rc={'figure.figsize': (10, 6)}, font_scale=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search_results = \"results_gridsearch/20171108_multiclass_results.txt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spacy is used for Part of Speech tagging and Named Entity Recognition\n",
    "# spacy is a non-standard python library which can be installed using 'pip install spacy' from the command line\n",
    "# language models can be downloaded by running 'python -m spacy download <language>' from the command line\n",
    "import spacy\n",
    "supported_languages = ['en','fr','es']\n",
    "language_dict = {'en':'english','fr':'french','es':'spanish'}\n",
    "nlp_langs = {}\n",
    "for language in supported_languages:\n",
    "    nlp_langs[language]  = spacy.load(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Downloaded xls file annotations from resourcecontracts and openlandcontracts\n",
    "resource_folder = 'contract_data/Contracts_Annotations/resource_contracts/'\n",
    "land_folder = 'contract_data/Contracts_Annotations/openland_contracts/'\n",
    "# Most recently downloaded metadata from resourcecontracts.org/contracts\n",
    "rc_metadata = 'contract_data/resource_contract_2017-08-16.csv' \n",
    "# Most recently downloaded metadata from openlandcontracts.org/contracts\n",
    "ol_metadata = 'contract_data/openland_contract_2017-08-16.csv'\n",
    "metadata_files = [rc_metadata,ol_metadata]\n",
    "folders = [resource_folder,land_folder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = ['stabilization','royalties']\n",
    "n_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of annotations: 20609\n"
     ]
    }
   ],
   "source": [
    "annotations = pd.DataFrame()\n",
    "for folder in folders:\n",
    "    xls_files = [f for f in os.listdir(folder) if f.lower().endswith('.xls')]\n",
    "    for xls in xls_files:\n",
    "        temp = pd.read_excel(folder + xls)\n",
    "        if len(temp) > 0:\n",
    "            temp['OCID'] = xls[:-4]\n",
    "            temp['Source'] = folder.split('/')[-2]\n",
    "            annotations = annotations.append(temp)\n",
    "print \"Number of annotations: \" + str(len(annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17885\n",
      "13795\n"
     ]
    }
   ],
   "source": [
    "# drop blank and integer annotations and annotations less than 4 words\n",
    "annotations.dropna(subset=['Annotation Text'],inplace=True)\n",
    "annotations = annotations[annotations['Annotation Text'].apply(lambda x: type(x)!=int)].copy()\n",
    "annotations = annotations[annotations['Annotation Text'].apply(lambda x: len(x.split()) > 3)].copy()\n",
    "print len(annotations)\n",
    "# If duplicate text appears within the same contract, drop it\n",
    "annotations.drop_duplicates(['Annotation Text','OCID','Category'],inplace=True)\n",
    "print len(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = [label.lower() for label in labels]\n",
    "annotations['label'] = [x.lower() if x.lower() in labels else 'other' for x in annotations['Category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13710\n"
     ]
    }
   ],
   "source": [
    "# Given some duplicate text with different Category labels, keep the labels that are in our target label list when dropping\n",
    "sort_num = range(len(labels) + 1)\n",
    "sort_key = dict(zip(labels,sort_num))\n",
    "sort_key['other'] = sort_num[-1]\n",
    "annotations['sort_key'] = [sort_key[x] for x in annotations['label']]\n",
    "annotations.sort_values(by='sort_key',inplace=True,ascending=True)\n",
    "annotations.drop_duplicates(['Annotation Text','OCID'],keep='first',inplace=True)\n",
    "print len(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Join metadata from contracts repository\n",
    "metadata = pd.DataFrame()\n",
    "for filename in metadata_files:\n",
    "    temp = pd.read_csv(filename)\n",
    "    metadata = metadata.append(temp)\n",
    "    \n",
    "annotations = annotations.merge(metadata[['OCID','Language','Country Name','Resource','Contract Type','Document Type']],how='left',on='OCID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.76% other\n",
      "2.44% royalties\n",
      "1.80% stabilization\n"
     ]
    }
   ],
   "source": [
    "y = list(annotations['label'])\n",
    "yunique = list(np.unique(y))\n",
    "for item in yunique:\n",
    "    print str(\"{0:.2f}%\".format(100*y.count(item) / float(len(y)))) + \" \" + item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Annotation Text</th>\n",
       "      <th>PDF Page Number</th>\n",
       "      <th>Article Reference</th>\n",
       "      <th>OCID</th>\n",
       "      <th>Source</th>\n",
       "      <th>label</th>\n",
       "      <th>sort_key</th>\n",
       "      <th>Language</th>\n",
       "      <th>Country Name</th>\n",
       "      <th>Resource</th>\n",
       "      <th>Contract Type</th>\n",
       "      <th>Document Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stabilization</td>\n",
       "      <td>Legal Rules</td>\n",
       "      <td>The government intends to present to the Natio...</td>\n",
       "      <td>4</td>\n",
       "      <td>Preamble</td>\n",
       "      <td>ocds-591adf-1834156729</td>\n",
       "      <td>resource_contracts</td>\n",
       "      <td>stabilization</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>Hydrocarbons</td>\n",
       "      <td>Production or Profit Sharing Agreement</td>\n",
       "      <td>Contract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stabilization</td>\n",
       "      <td>Legal Rules</td>\n",
       "      <td>Le gouvernement s'engage aussi à garantir à SG...</td>\n",
       "      <td>17</td>\n",
       "      <td>Art. 11.1</td>\n",
       "      <td>ocds-591adf-9069819553</td>\n",
       "      <td>resource_contracts</td>\n",
       "      <td>stabilization</td>\n",
       "      <td>0</td>\n",
       "      <td>fr</td>\n",
       "      <td>Senegal</td>\n",
       "      <td>Gold;Silver</td>\n",
       "      <td>Concession Agreement</td>\n",
       "      <td>Contract</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Category        Topic  \\\n",
       "0  Stabilization  Legal Rules   \n",
       "1  Stabilization  Legal Rules   \n",
       "\n",
       "                                     Annotation Text  PDF Page Number  \\\n",
       "0  The government intends to present to the Natio...                4   \n",
       "1  Le gouvernement s'engage aussi à garantir à SG...               17   \n",
       "\n",
       "  Article Reference                    OCID              Source  \\\n",
       "0          Preamble  ocds-591adf-1834156729  resource_contracts   \n",
       "1         Art. 11.1  ocds-591adf-9069819553  resource_contracts   \n",
       "\n",
       "           label  sort_key Language Country Name      Resource  \\\n",
       "0  stabilization         0       en         Iraq  Hydrocarbons   \n",
       "1  stabilization         0       fr      Senegal   Gold;Silver   \n",
       "\n",
       "                            Contract Type Document Type  \n",
       "0  Production or Profit Sharing Agreement      Contract  \n",
       "1                    Concession Agreement      Contract  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def punctuation_remove(text):\n",
    "    \"\"\"\n",
    "    Mutates and returns text where all punctuation besides underscores,\n",
    "    are replaced\n",
    "    \"\"\"\n",
    "    punctuation_re = r'[^\\w\\s_]'\n",
    "    new_text = nltk.regexp.re.sub(punctuation_re, ' ', text)\n",
    "    return new_text\n",
    "\n",
    "def replace_numbers(text):\n",
    "    ''' \n",
    "    Removes all characters but periods, commas and alpha-numeric and \n",
    "    returns all numeric values replace with the word numeric_value\n",
    "    '''\n",
    "#     allowed = {\",\", \".\",\" \",\"%\"}.union(string.ascii_letters).union([str(num) for num in range(0,10)])\n",
    "#     filtered = ''.join([character for character in text if character in allowed])\n",
    "    wordlist = text.split()\n",
    "    for i in range(len(wordlist)):\n",
    "        if '$' in wordlist[i]:\n",
    "            try:\n",
    "                int(wordlist[i].split('$')[-1].replace(',','').replace('.','').replace('-','').replace(')','').replace('(','').replace('\\'','').replace(';','').replace(':',''))\n",
    "                wordlist[i] = ' '.join(wordlist[i].split('$')[:-1]) + ' dollarvalue'\n",
    "            except:\n",
    "                pass\n",
    "        elif u'\\xb0' in wordlist[i]:\n",
    "            wordlist[i] == 'degreevalue'\n",
    "        elif '%' in wordlist[i]:\n",
    "            try: \n",
    "                int(wordlist[i].replace(',','').replace('.','').replace('-','').replace('/','').replace('%','').replace(')','').replace('(','').replace('\\'','').replace(';','').replace(':',''))\n",
    "                wordlist[i] = 'percentvalue'\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            try: \n",
    "                int(wordlist[i].replace(',','').replace('.','').replace('-','').replace('/','').replace(')','').replace('(','').replace('\\'','').replace(';','').replace(':',''))\n",
    "                wordlist[i] = 'numericvalue'\n",
    "            except:\n",
    "                pass\n",
    "    return ' '.join(wordlist)\n",
    "\n",
    "def perform_lowercase(text):\n",
    "    \"\"\"\n",
    "    Mutates and returns text where all characters are lowercased\n",
    "    \"\"\"\n",
    "    try:\n",
    "        new_text = text.lower()\n",
    "    except:\n",
    "        new_text = str(text).lower()\n",
    "    return new_text\n",
    "\n",
    "def underscore_remove(text):\n",
    "    '''\n",
    "    replaces multiple underscores with text fillintheblank\n",
    "    and single underscore with space\n",
    "    '''\n",
    "    double_underscore_re = r'(__[a-zA-Z0-9_]*(__)?)'\n",
    "    text = nltk.regexp.re.sub(double_underscore_re,'fillintheblank',text)\n",
    "    return text.replace('_',' ')\n",
    "\n",
    "def doublespace_remove(text):\n",
    "    return re.sub(' +',' ',text)\n",
    "\n",
    "def textblobsent(text):\n",
    "    '''\n",
    "    returns the TextBlob polarity and subjectivity\n",
    "    '''\n",
    "    text = text.encode('ascii','replace')\n",
    "    sent = TextBlob(text).sentiment\n",
    "    return pd.Series([sent.polarity,sent.subjectivity])\n",
    "\n",
    "def get_avg_wordlength(document):\n",
    "    wordlengths = [len(word) for word in document.split()]\n",
    "    if len(wordlengths) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.mean(wordlengths)\n",
    "\n",
    "def get_multilingual_pos(text,language):\n",
    "    if language in nlp_langs.keys():\n",
    "        try:\n",
    "            tokens = nlp_langs[language](text)\n",
    "            tags = [token.pos_ for token in tokens]\n",
    "            counts = Counter(tags).items()\n",
    "            countdict = {}\n",
    "            for key, value in counts:\n",
    "                countdict[key] = value\n",
    "            return countdict\n",
    "        except:\n",
    "            return {'pos_error':1}\n",
    "    else:\n",
    "        return {'unsupported_language':1}\n",
    "\n",
    "def get_multilingual_entities(row):\n",
    "    \n",
    "    text = row['CleanText']\n",
    "    language = row['Language']\n",
    "    if language in nlp_langs.keys():\n",
    "        try:\n",
    "            doc = nlp_langs[language](text)\n",
    "            labels = [ent.label_ for ent in doc.ents]\n",
    "            texts = [ent.text for ent in doc.ents]\n",
    "            starts = [ent.start_char for ent in doc.ents]\n",
    "            ends = [ent.end_char for ent in doc.ents]\n",
    "            textlens = [len(word) for word in texts]\n",
    "            labellens = [len(label) for label in labels]\n",
    "            diffs = [textlens[i] - labellens[i] for i in range(len(textlens))]\n",
    "            diffsum = [sum(diffs[0:i]) for i in range(len(diffs))]\n",
    "\n",
    "            for i in range(len(labels)):\n",
    "                text = text[0:starts[i] - diffsum[i]] + 'entity' + labels[i] + text[ends[i] - diffsum[i]:]\n",
    "            return text\n",
    "        except:\n",
    "            return text\n",
    "    else:\n",
    "        return text\n",
    "    \n",
    "def remove_stopwords(row):\n",
    "    '''\n",
    "    Multilingual stopwords removal\n",
    "    '''\n",
    "    try:\n",
    "        language = language_dict[row['Language']]\n",
    "        text = ' '.join([word for word in row['CleanText'].split(' ') if word not in stopwords.words(language)])\n",
    "        return text\n",
    "    except:\n",
    "        return row['CleanText']\n",
    "    \n",
    "def stem_words(row):\n",
    "    ''' \n",
    "    Multilingual word stemmer\n",
    "    '''\n",
    "    language = language_dict[row['Language']]\n",
    "    try:\n",
    "        stemmer = SnowballStemmer(language)\n",
    "        text = ' '.join([stemmer.stem(word) for word in row['CleanText_NoStop'].split(' ')])\n",
    "        return text\n",
    "    except:\n",
    "        return row['CleanText_NoStop']\n",
    "    \n",
    "def clean_metadata(text):\n",
    "    if type(text) in [float,int]:\n",
    "        return text\n",
    "    elif type(text) == str:\n",
    "        return text.lower().split(';')[0]\n",
    "    else:\n",
    "        text = text.encode('ascii','replace')\n",
    "        return text.lower().split(';')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(df):\n",
    "    df['Annotation Text'].fillna('',inplace=True)\n",
    "    df['CleanText'] = df['Annotation Text']\n",
    "    df['CleanText'] = df.apply(get_multilingual_entities,axis=1)\n",
    "    func_list = [perform_lowercase,replace_numbers,punctuation_remove,underscore_remove, doublespace_remove]\n",
    "    for func in func_list:\n",
    "        df['CleanText'] = df['CleanText'].apply(func)\n",
    "\n",
    "    return df\n",
    "\n",
    "def featurize(df):\n",
    "    \n",
    "    df['AvgWordLength'] = df['CleanText'].apply(get_avg_wordlength)\n",
    "    \n",
    "    df['CleanText_NoStop'] = df.apply(remove_stopwords,axis=1)\n",
    "    df['CleanText_NoStop_Stemmed'] = df.apply(stem_words,axis=1)\n",
    "    \n",
    "    postagcounts = []\n",
    "    entitycounts = []\n",
    "    for index, row in df.iterrows():\n",
    "        postagcounts.append(get_multilingual_pos(row['Annotation Text'],row['Language']))    \n",
    "    postagdf = pd.DataFrame(postagcounts).fillna(0)\n",
    "    postagdf.index = df.index\n",
    "    postagdf.columns = ['postag_' + col for col in postagdf.columns]\n",
    "    \n",
    "    # create dummy variables for categoricals\n",
    "    df['Resource'] = df['Resource'].apply(clean_metadata)\n",
    "    df['Contract Type'] = df['Contract Type'].apply(clean_metadata)\n",
    "    df['Document Type'] = df['Document Type'].apply(lambda x: x.lower().split(';')[0])\n",
    "    dummy_cols = ['Language','Country Name','Resource','Contract Type','Document Type']\n",
    "    dummies = pd.get_dummies(df[dummy_cols],prefix = dummy_cols)\n",
    "    # drop lowest least frequent dummy columns for each\n",
    "    for dummy_col in dummy_cols:\n",
    "        cols = [col for col in dummies.columns if col.startswith(dummy_col)]\n",
    "        dummies.drop([col for col, val in dummies[cols].sum().iteritems() if val == dummies[cols].sum().min()],axis=1,inplace=True)\n",
    "    df.drop(dummy_cols, axis=1,inplace=True)\n",
    "        \n",
    "    textblobsentdf = df['CleanText'].apply(textblobsent)\n",
    "    textblobsentdf.columns = ['TextblobPolarity','TextblobSubjectivity']\n",
    "    df = pd.concat([df,textblobsentdf,postagdf,dummies],axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "annotations = clean_text(annotations)\n",
    "annotations = featurize(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13710, 8449)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df= .6,\n",
    "                                 min_df= .002, \n",
    "                                 stop_words=None,  \n",
    "                                 use_idf=True, \n",
    "                                 ngram_range=(1,5))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(annotations['CleanText_NoStop_Stemmed'].values.astype('U'))\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "tfidf_matrix = tfidf_matrix.todense()\n",
    "tfidf = pd.DataFrame(tfidf_matrix)\n",
    "tfidf.index = annotations.index\n",
    "tfidf.columns = terms\n",
    "print tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exclude = ['Source','Category','Topic','Annotation Text','CleanText','CleanText_NoStop','CleanText_NoStop_Stemmed',\n",
    "           'OCID','PDF Page Number','Article Reference','MD','VBP','VBZ','VBG','VBD','VBN','other',\"''\",'label','sort_key']\n",
    "features = [str(col) for col in annotations.columns.tolist() if not col in exclude]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.concat([tfidf,annotations[features]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.fillna(0,inplace=True)\n",
    "X = X.rename(columns = {'fit':'fit_feature'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "#             LogisticRegression(),\n",
    "#             DecisionTreeClassifier(),\n",
    "            RandomForestClassifier()\n",
    "#             ExtraTreesClassifier()\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = [\n",
    "#         {'penalty': ['l1','l2']      # LogisticRegression\n",
    "#               ,'C': [.001,.01,.1,1,10,100]\n",
    "#               ,'class_weight': [None,'balanced']\n",
    "#               ,'n_jobs': [-1]}\n",
    "#         ,{'max_depth': [8,10,12,14]}       #DecisionTree\n",
    "         {'estimator__n_estimators': [500],          # RandomForest\n",
    "          'estimator__n_jobs': [-1],\n",
    "            \"estimator__max_depth\": [None,12,14],\n",
    "              \"estimator__max_features\": [.2,.33],\n",
    "              \"estimator__min_samples_split\": [2,10],\n",
    "              \"estimator__min_samples_leaf\": [50],\n",
    "              \"estimator__bootstrap\": [False,True],\n",
    "              \"estimator__criterion\": ['gini'],\n",
    "              \"estimator__class_weight\": ['balanced','balanced_subsample']}\n",
    "#         ,{'max_features':[0.2,0.33,0.5]       #ExtraTrees\n",
    "#               ,'n_estimators': [50]\n",
    "#               ,'n_jobs': [-1]\n",
    "#               ,'max_depth': [3,6,9]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def grid_search(X_all, y_all, classifiers, parameters):\n",
    "    \n",
    "    # Runs grid search on given list of classifiers and parameters dictionary\n",
    "    scores = ['recall_macro']\n",
    "    \n",
    "    results = []\n",
    "    for i in range(len(classifiers)):\n",
    "        model = str(classifiers[i]).split('(')[:1][0]\n",
    "        print '     ******************************************'\n",
    "        print '     *** ' + model + ' ***'        \n",
    "        print '     Tuning hyper-parameters for:' \n",
    "        for score in scores:\n",
    "            print '     ' + score.title()\n",
    "            clf = GridSearchCV(OneVsRestClassifier(classifiers[i]), parameters[i],cv=n_folds,scoring=score, verbose=10)            \n",
    "            clf.fit(X_all,y_all)\n",
    "            y_true, y_pred = y_all, clf.predict(X_all)\n",
    "            \n",
    "            print \"Accuracy Score: \" + str(accuracy_score(y_true, y_pred))\n",
    "            print classification_report(y_true, y_pred, target_names=yunique)\n",
    "            result = [model, score, clf.best_params_, clf.best_score_]\n",
    "            results.append(result)\n",
    "            with open(grid_search_results, \"a\") as myfile:\n",
    "                myfile.write(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ******************************************\n",
      "     *** RandomForestClassifier ***\n",
      "     Tuning hyper-parameters for:\n",
      "     Recall_Macro\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "[CV] estimator__min_samples_leaf=50, estimator__criterion=gini, estimator__class_weight=balanced, estimator__min_samples_split=2, estimator__max_features=0.2, estimator__bootstrap=False, estimator__max_depth=None, estimator__n_estimators=500, estimator__n_jobs=-1 \n"
     ]
    }
   ],
   "source": [
    "label_results = grid_search(X, y, classifiers, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth',500)\n",
    "resultsdf = pd.DataFrame(label_results)\n",
    "resultsdf.columns = ['Model','Scoring Method','Best Params','Recall']\n",
    "resultsdf.to_csv('Multiclass_grid_search_results.csv')\n",
    "display(resultsdf.sort_values(by='Recall',ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
