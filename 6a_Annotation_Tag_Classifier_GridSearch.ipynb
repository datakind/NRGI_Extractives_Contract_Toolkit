{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](DataKind_orange.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performs grid search for binary classification of Stabilization and Royalties clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, io\n",
    "from collections import Counter\n",
    "from itertools import product\n",
    "import requests\n",
    "import re, string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from textblob import TextBlob\n",
    "from textstat.textstat import textstat\n",
    "from langdetect import detect\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook', rc={'figure.figsize': (10, 6)}, font_scale=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resource_folder = 'contract_data/Contracts_Annotations/resource_contracts/'\n",
    "land_folder = 'contract_data/Contracts_Annotations/openland_contracts/'\n",
    "folders = [resource_folder,land_folder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = ['stabilization','royalties']\n",
    "n_folds = 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Languages must be supported by NLTK Snowball Stemmer and stopwords\n",
    "languages = ['english','french','spanish','italian','portuguese','dutch','swedish']\n",
    "s = requests.get('http://data.okfn.org/data/core/language-codes/r/language-codes.csv').content\n",
    "language_codes = pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    "language_codes.columns = ['language_code','Language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of annotations: 20609\n"
     ]
    }
   ],
   "source": [
    "annotations = pd.DataFrame()\n",
    "for folder in folders:\n",
    "    xls_files = [f for f in os.listdir(folder) if f.lower().endswith('.xls')]\n",
    "    for xls in xls_files:\n",
    "        temp = pd.read_excel(folder + xls)\n",
    "        if len(temp) > 0:\n",
    "            temp['OCID'] = xls[:-4]\n",
    "            temp['Source'] = folder.split('/')[-2]\n",
    "            annotations = annotations.append(temp)\n",
    "print \"Number of annotations: \" + str(len(annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20561\n",
      "16364\n"
     ]
    }
   ],
   "source": [
    "# drop blank annotations\n",
    "annotations.dropna(subset=['Annotation Text'],inplace=True)\n",
    "print len(annotations)\n",
    "# If duplicate text appears within the same contract, drop it\n",
    "annotations.drop_duplicates(['Annotation Text','OCID','Category'],inplace=True)\n",
    "print len(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = [label.lower() for label in labels]\n",
    "annotations['label'] = [x.lower() if x.lower() in labels else 'other' for x in annotations['Category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16181\n"
     ]
    }
   ],
   "source": [
    "# Given some duplicate text with different Category labels, keep the labels that are in our target label list when dropping\n",
    "sort_num = range(len(labels) + 1)\n",
    "sort_key = dict(zip(labels,sort_num))\n",
    "sort_key['other'] = sort_num[-1]\n",
    "annotations['sort_key'] = [sort_key[x] for x in annotations['label']]\n",
    "annotations.sort_values(by='sort_key',inplace=True,ascending=True)\n",
    "annotations.drop_duplicates(['Annotation Text','OCID'],keep='first',inplace=True)\n",
    "print len(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.40% other\n",
      "2.07% royalties\n",
      "1.53% stabilization\n"
     ]
    }
   ],
   "source": [
    "y = list(annotations['label'])\n",
    "yunique = list(np.unique(y))\n",
    "for item in yunique:\n",
    "    print str(\"{0:.2f}%\".format(100*y.count(item) / float(len(y)))) + \" \" + item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Annotation Text</th>\n",
       "      <th>PDF Page Number</th>\n",
       "      <th>Article Reference</th>\n",
       "      <th>OCID</th>\n",
       "      <th>Source</th>\n",
       "      <th>label</th>\n",
       "      <th>sort_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Stabilization</td>\n",
       "      <td>Legal Rules</td>\n",
       "      <td>If, as a result of any change in the laws, rul...</td>\n",
       "      <td>30</td>\n",
       "      <td>Art. 18. 3</td>\n",
       "      <td>ocds-591adf-1990915353</td>\n",
       "      <td>resource_contracts</td>\n",
       "      <td>stabilization</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Stabilization</td>\n",
       "      <td>Legal Rules</td>\n",
       "      <td>If a material change occurs to Phillips China ...</td>\n",
       "      <td>40</td>\n",
       "      <td>Art. 27.2</td>\n",
       "      <td>ocds-591adf-1706855956</td>\n",
       "      <td>resource_contracts</td>\n",
       "      <td>stabilization</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Category        Topic  \\\n",
       "23  Stabilization  Legal Rules   \n",
       "29  Stabilization  Legal Rules   \n",
       "\n",
       "                                      Annotation Text  PDF Page Number  \\\n",
       "23  If, as a result of any change in the laws, rul...               30   \n",
       "29  If a material change occurs to Phillips China ...               40   \n",
       "\n",
       "   Article Reference                    OCID              Source  \\\n",
       "23        Art. 18. 3  ocds-591adf-1990915353  resource_contracts   \n",
       "29         Art. 27.2  ocds-591adf-1706855956  resource_contracts   \n",
       "\n",
       "            label  sort_key  \n",
       "23  stabilization         0  \n",
       "29  stabilization         0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def punctuation_remove(text):\n",
    "    \"\"\"\n",
    "    Mutates and returns text where all punctuation besides underscores,\n",
    "    are replaced\n",
    "    \"\"\"\n",
    "    punctuation_re = r'[^\\w\\s_]'\n",
    "    new_text = nltk.regexp.re.sub(punctuation_re, ' ', text)\n",
    "    return new_text\n",
    "\n",
    "def replace_numbers(text):\n",
    "    ''' \n",
    "    Removes all characters but periods, commas and alpha-numeric and \n",
    "    returns all numeric values replace with the word numeric_value\n",
    "    '''\n",
    "    allowed = {\",\", \".\",\" \"}.union(string.ascii_letters).union([str(num) for num in range(0,10)])\n",
    "    filtered = ''.join([character for character in text if character in allowed])\n",
    "    for word in text.split():\n",
    "        try:\n",
    "            int(word.replace(',','').replace('.',''))\n",
    "            text = text.replace(word,'numericvalue')\n",
    "        except:\n",
    "            pass\n",
    "    return text\n",
    "\n",
    "def perform_lowercase(text):\n",
    "    \"\"\"\n",
    "    Mutates and returns text where all characters are lowercased\n",
    "    \"\"\"\n",
    "    try:\n",
    "        new_text = text.lower()\n",
    "    except:\n",
    "        new_text = str(text).lower()\n",
    "    return new_text\n",
    "\n",
    "def underscore_remove(text):\n",
    "    '''\n",
    "    replaces multiple underscores with text fillintheblank\n",
    "    and single underscore with space\n",
    "    '''\n",
    "    double_underscore_re = r'(__[a-zA-Z0-9_]*(__)?)'\n",
    "    text = nltk.regexp.re.sub(double_underscore_re,'fillintheblank',text)\n",
    "    return text.replace('_',' ')\n",
    "\n",
    "def doublespace_remove(text):\n",
    "    return re.sub(' +',' ',text)\n",
    "\n",
    "def textblobsent(text):\n",
    "    '''\n",
    "    returns the TextBlob polarity and subjectivity\n",
    "    '''\n",
    "    text = text.encode('ascii','replace')\n",
    "    sent = TextBlob(text).sentiment\n",
    "    return pd.Series([sent.polarity,sent.subjectivity])\n",
    "\n",
    "def get_avg_wordlength(document):\n",
    "    wordlengths = [len(word) for word in document.split()]\n",
    "    if len(wordlengths) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.mean(wordlengths)\n",
    "\n",
    "def determine_tense(essay):\n",
    "    '''\n",
    "    Returns the number of past, present and future tense verbs in a given text\n",
    "    '''\n",
    "    text = word_tokenize(essay)\n",
    "    tagged = pos_tag(text)\n",
    "\n",
    "    numfuture = len([word for word in tagged if word[1] == \"MD\"])\n",
    "    numpresent = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n",
    "    numpast = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]]) \n",
    "    \n",
    "    return pd.Series([numpast,numpresent,numfuture],index=['NumPast','NumPresent','NumFuture'])\n",
    "\n",
    "def get_pos_tags(text):\n",
    "    '''\n",
    "    Returns part of speech tag counts\n",
    "    '''\n",
    "    text = word_tokenize(text)\n",
    "    tagged = pos_tag(text)\n",
    "\n",
    "    counts = Counter([word[1] for word in tagged]).items()\n",
    "    countdict = {}\n",
    "    for key, value in counts:\n",
    "        countdict[key] = value  \n",
    "    \n",
    "    return countdict\n",
    "\n",
    "def detect_lang(text):\n",
    "    '''\n",
    "    Returns detected language\n",
    "    '''\n",
    "    text = doublespace_remove(text)\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return 'None'\n",
    "    \n",
    "def remove_stopwords(row):\n",
    "    '''\n",
    "    Multilingual stopwords removal\n",
    "    '''\n",
    "    language = row['Language']\n",
    "    if language in languages:\n",
    "        text = ' '.join([word for word in row['CleanText'].split(' ') if word not in stopwords.words(language)])\n",
    "        return text\n",
    "    else:\n",
    "        return row['CleanText']\n",
    "    \n",
    "def stem_words(row):\n",
    "    ''' \n",
    "    Multilingual word stemmer\n",
    "    '''\n",
    "    language = row['Language']    \n",
    "    if language in languages:\n",
    "        stemmer = SnowballStemmer(language)\n",
    "        text = ' '.join([stemmer.stem(word) for word in row['CleanText_NoStop'].split(' ')])\n",
    "        return text\n",
    "    else:\n",
    "        return row['CleanText_NoStop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(df):\n",
    "    df['Annotation Text'].fillna('',inplace=True)\n",
    "    df['CleanText'] = df['Annotation Text']\n",
    "    func_list = [perform_lowercase,replace_numbers,punctuation_remove,underscore_remove, doublespace_remove]\n",
    "    for func in func_list:\n",
    "        df['CleanText'] = df['CleanText'].apply(func)\n",
    "\n",
    "    return df\n",
    "\n",
    "def featurize(df):\n",
    "    \n",
    "    df['AvgWordLength'] = df['CleanText'].apply(get_avg_wordlength)\n",
    "    df['language_code'] = df['CleanText'].apply(detect_lang)\n",
    "    df = df.merge(language_codes,how='left',on='language_code')\n",
    "    df['Language'] = df['Language'].astype('str')\n",
    "    df['Language'] = df['Language'].apply(lambda x: x.lower().split(';')[0])\n",
    "    df['Language'] = df['Language'].apply(lambda x: x if x in languages else 'other')\n",
    "    df['Language'].fillna('None')\n",
    "    langdummies = pd.get_dummies(df['Language'],prefix='language')\n",
    "    \n",
    "    df['CleanText_NoStop'] = df.apply(remove_stopwords,axis=1)\n",
    "    df['CleanText_NoStop_Stemmed'] = df.apply(stem_words,axis=1)\n",
    "    df.drop(['language_code','Language'],axis=1,inplace=True)\n",
    "    \n",
    "    tenses = df['CleanText'].apply(determine_tense)\n",
    "    tenses.columns = ['tense_' + col for col in tenses.columns]\n",
    "    \n",
    "    postagcounts = []\n",
    "    for index, row in df.iterrows():\n",
    "        postagcounts.append(get_pos_tags(row['CleanText']))    \n",
    "    postagdf = pd.DataFrame(postagcounts).fillna(0)\n",
    "    postagdf.index = df.index\n",
    "    postagdf.columns = ['postag_' + col for col in postagdf.columns]\n",
    "    \n",
    "    textblobsentdf = df['CleanText'].apply(textblobsent)\n",
    "    textblobsentdf.columns = ['TextblobPolarity','TextblobSubjectivity']\n",
    "    df = pd.concat([df,textblobsentdf,tenses,postagdf,langdummies],axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "annotations = clean_text(annotations)\n",
    "annotations = featurize(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16181, 6551)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df= .6,\n",
    "                                 min_df= .002, \n",
    "                                 stop_words=None,  \n",
    "                                 use_idf=True, \n",
    "                                 ngram_range=(1,4))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(annotations['CleanText_NoStop_Stemmed'].values.astype('U'))\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "tfidf_matrix = tfidf_matrix.todense()\n",
    "tfidf = pd.DataFrame(tfidf_matrix)\n",
    "tfidf.index = annotations.index\n",
    "tfidf.columns = terms\n",
    "print tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exclude = ['Source','Category','Topic','Annotation Text','CleanText','CleanText_NoStop','CleanText_NoStop_Stemmed',\n",
    "           'OCID','PDF Page Number','Article Reference','MD','VBP','VBZ','VBG','VBD','VBN','other',\"''\",'label','sort_key']\n",
    "features = [col for col in annotations.columns.tolist() if not col in exclude]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.concat([tfidf,annotations[features]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.fillna(0,inplace=True)\n",
    "X = X.rename(columns = {'fit':'fit_feature'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "#             LogisticRegression(),\n",
    "#             DecisionTreeClassifier(),\n",
    "            RandomForestClassifier()\n",
    "#             ExtraTreesClassifier()\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = [\n",
    "#         {'penalty': ['l1','l2']      # LogisticRegression\n",
    "#               ,'C': [.001,.01,.1,1,10,100]\n",
    "#               ,'class_weight': [None,'balanced']\n",
    "#               ,'n_jobs': [-1]}\n",
    "#         ,{'max_depth': [8,10,12,14]}       #DecisionTree\n",
    "         {'estimator__n_estimators': [500],          # RandomForest\n",
    "          'estimator__n_jobs': [-1],\n",
    "            \"estimator__max_depth\": [3, 6, 9],\n",
    "              \"estimator__max_features\": [.2,.33,.5],\n",
    "              \"estimator__min_samples_split\": [2, 4, 10],\n",
    "              \"estimator__min_samples_leaf\": [2, 4, 10],\n",
    "              \"estimator__bootstrap\": [True, False],\n",
    "              \"estimator__criterion\": [\"gini\", \"entropy\"],\n",
    "              \"estimator__class_weight\": ['balanced','balanced_subsample']}\n",
    "#         ,{'max_features':[0.2,0.33,0.5]       #ExtraTrees\n",
    "#               ,'n_estimators': [50]\n",
    "#               ,'n_jobs': [-1]\n",
    "#               ,'max_depth': [3,6,9]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def grid_search(X_all, y_all, classifiers, parameters):\n",
    "    \n",
    "    # Runs grid search on given list of classifiers and parameters dictionary\n",
    "    scores = ['recall_macro']\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.33)\n",
    "    \n",
    "    results = []\n",
    "    for i in range(len(classifiers)):\n",
    "        model = str(classifiers[i]).split('(')[:1][0]\n",
    "        print '     ******************************************'\n",
    "        print '     *** ' + model + ' ***'        \n",
    "        print '     Tuning hyper-parameters for:' \n",
    "        for score in scores:\n",
    "            print '     ' + score.title()\n",
    "#             clf = GridSearchCV(classifiers[i], parameters[i],cv=n_folds,scoring=score)            \n",
    "            clf = GridSearchCV(OneVsRestClassifier(classifiers[i]), parameters[i],cv=n_folds,scoring=score)            \n",
    "            clf.fit(X_all,y_all)\n",
    "            y_true, y_pred = y_all, clf.predict(X_all)\n",
    "            \n",
    "#             acc = accuracy_score(y_true, y_pred)\n",
    "#             prec = precision_score(y_true, y_pred, average='macro')\n",
    "#             rec = recall_score(y_true, y_pred, average='macro')\n",
    "#             roc_auc = roc_auc_score(y_true, y_pred, average='macro')\n",
    "            print classification_report(y_true, y_pred, target_names=clf.classes_)\n",
    "            results.append([model, score, clf.best_params_, clf.best_score_])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ******************************************\n",
      "     *** RandomForestClassifier ***\n",
      "     Tuning hyper-parameters for:\n",
      "     Recall_Macro\n"
     ]
    }
   ],
   "source": [
    "label_results = grid_search(X, y, classifiers, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(label_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth',200)\n",
    "resultsdf = pd.DataFrame(label_results)\n",
    "resultsdf.columns = ['Model','Scoring Method','Best Params','Recall']\n",
    "resultsdf.to_csv('Multiclass_grid_search_results.csv')\n",
    "display(resultsdf.sort_values(by='Recall',ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
