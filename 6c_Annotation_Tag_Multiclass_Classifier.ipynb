{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](DataKind_orange.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classifier for contract clause types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "from collections import Counter\n",
    "from itertools import product\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from textblob import TextBlob\n",
    "from textstat.textstat import textstat\n",
    "from langdetect import detect\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook', rc={'figure.figsize': (10, 6)}, font_scale=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resource_folder = 'contract_data/Contracts_Annotations/resource_contracts/'\n",
    "land_folder = 'contract_data/Contracts_Annotations/openland_contracts/'\n",
    "folders = [resource_folder,land_folder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = ['stabilization','royalties']\n",
    "n_folds = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Languages must be supported by NLTK Snowball Stemmer and stopwords\n",
    "languages = ['english','french','spanish','italian','portuguese','dutch','swedish']\n",
    "language_codes = pd.read_csv('http://data.okfn.org/data/core/language-codes/r/language-codes.csv')\n",
    "language_codes.columns = ['language_code','Language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = OneVsRestClassifier(RandomForestClassifier(\n",
    "                                                    n_estimators=50,\n",
    "                                                    max_features=.33, \n",
    "                                                    max_depth=6, \n",
    "                                                    n_jobs=-1, \n",
    "                                                    oob_score=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of annotations: 20609\n"
     ]
    }
   ],
   "source": [
    "annotations = pd.DataFrame()\n",
    "for folder in folders:\n",
    "    xls_files = [f for f in os.listdir(folder) if f.lower().endswith('.xls')]\n",
    "    for xls in xls_files:\n",
    "        temp = pd.read_excel(folder + xls)\n",
    "        if len(temp) > 0:\n",
    "            temp['OCID'] = xls[:-4]\n",
    "            temp['Source'] = folder.split('/')[-2]\n",
    "            annotations = annotations.append(temp)\n",
    "print \"Number of annotations: \" + str(len(annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.08% other\n",
      "2.37% royalties\n",
      "1.54% stabilization\n"
     ]
    }
   ],
   "source": [
    "labels = [label.lower() for label in labels]\n",
    "y = [x.lower() if x.lower() in labels else 'other' for x in annotations['Category']]\n",
    "yunique = list(np.unique(y))\n",
    "for item in yunique:\n",
    "    print str(\"{0:.2f}%\".format(100*y.count(item) / float(len(y)))) + \" \" + item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Annotation Text</th>\n",
       "      <th>PDF Page Number</th>\n",
       "      <th>Article Reference</th>\n",
       "      <th>OCID</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Project title</td>\n",
       "      <td>General</td>\n",
       "      <td>Production Sharing Contract between Sociedade ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Preamble</td>\n",
       "      <td>ocds-591adf-0014595575</td>\n",
       "      <td>resource_contracts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Name of field, block, deposit or site</td>\n",
       "      <td>General</td>\n",
       "      <td>Block 20/11</td>\n",
       "      <td>1</td>\n",
       "      <td>Preamble</td>\n",
       "      <td>ocds-591adf-0014595575</td>\n",
       "      <td>resource_contracts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Category    Topic  \\\n",
       "0                          Project title  General   \n",
       "1  Name of field, block, deposit or site  General   \n",
       "\n",
       "                                     Annotation Text  PDF Page Number  \\\n",
       "0  Production Sharing Contract between Sociedade ...                1   \n",
       "1                                        Block 20/11                1   \n",
       "\n",
       "  Article Reference                    OCID              Source  \n",
       "0          Preamble  ocds-591adf-0014595575  resource_contracts  \n",
       "1          Preamble  ocds-591adf-0014595575  resource_contracts  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punctuation_re = r'[^\\w\\s#]'\n",
    "\n",
    "def punctuation_remove(text):\n",
    "    \"\"\"\n",
    "    Mutates and returns text where all punctuation besides hashtags,\n",
    "    are replaced\n",
    "    \"\"\"\n",
    "    new_text = nltk.regexp.re.sub(punctuation_re, ' ', text)\n",
    "    return new_text\n",
    "\n",
    "def perform_lowercase(text):\n",
    "    \"\"\"\n",
    "    Mutates and returns text where all characters are lowercased\n",
    "    \"\"\"\n",
    "    try:\n",
    "        new_text = text.lower()\n",
    "    except:\n",
    "        new_text = str(text).lower()\n",
    "    return new_text\n",
    "\n",
    "def doublespace_remove(text):\n",
    "    return re.sub(' +',' ',text)\n",
    "\n",
    "def textblobsent(text):\n",
    "    '''\n",
    "    returns the TextBlob polarity and subjectivity\n",
    "    '''\n",
    "    text = text.encode('ascii','ignore')\n",
    "    sent = TextBlob(text).sentiment\n",
    "    return pd.Series([sent.polarity,sent.subjectivity])\n",
    "\n",
    "def get_length(document):\n",
    "    return len(document)\n",
    "\n",
    "def get_num_words(document):\n",
    "    return len(document.split())\n",
    "\n",
    "def get_avg_wordlength(document):\n",
    "    wordlengths = [len(word) for word in document.split()]\n",
    "    if len(wordlengths) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.mean(wordlengths)\n",
    "\n",
    "def get_num_syllables(text):\n",
    "    '''\n",
    "    returns the number of syllables\n",
    "    '''\n",
    "    return textstat.syllable_count(text)\n",
    "\n",
    "def determine_tense(essay):\n",
    "    '''\n",
    "    Returns the number of past, present and future tense verbs in a given text\n",
    "    '''\n",
    "    text = word_tokenize(essay)\n",
    "    tagged = pos_tag(text)\n",
    "\n",
    "    numfuture = len([word for word in tagged if word[1] == \"MD\"])\n",
    "    numpresent = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n",
    "    numpast = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]]) \n",
    "    \n",
    "    return pd.Series([numpast,numpresent,numfuture],index=['NumPast','NumPresent','NumFuture'])\n",
    "\n",
    "def get_pos_tags(text):\n",
    "    '''\n",
    "    Returns part of speech tag counts\n",
    "    '''\n",
    "    text = word_tokenize(text)\n",
    "    tagged = pos_tag(text)\n",
    "\n",
    "    counts = Counter([word[1] for word in tagged]).items()\n",
    "    countdict = {}\n",
    "    for key, value in counts:\n",
    "        countdict[key] = value  \n",
    "    \n",
    "    return countdict\n",
    "\n",
    "def detect_lang(text):\n",
    "    '''\n",
    "    Returns detected language\n",
    "    '''\n",
    "    text = doublespace_remove(text)\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return 'None'\n",
    "    \n",
    "def remove_stopwords(row):\n",
    "    '''\n",
    "    Multilingual stopwords removal\n",
    "    '''\n",
    "    language = row['Language']\n",
    "    if language in languages:\n",
    "        text = ' '.join([word for word in row['CleanText'].split(' ') if word not in stopwords.words(language)])\n",
    "        return text\n",
    "    else:\n",
    "        return row['CleanText']\n",
    "    \n",
    "def stem_words(row):\n",
    "    ''' \n",
    "    Multilingual word stemmer\n",
    "    '''\n",
    "    language = row['Language']    \n",
    "    if language in languages:\n",
    "        stemmer = SnowballStemmer(language)\n",
    "        text = ' '.join([stemmer.stem(word) for word in row['CleanText_NoStop'].split(' ')])\n",
    "        return text\n",
    "    else:\n",
    "        return row['CleanText_NoStop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(df):\n",
    "    df['Annotation Text'].fillna('',inplace=True)\n",
    "    df['CleanText'] = df['Annotation Text']\n",
    "    func_list = [perform_lowercase,punctuation_remove,doublespace_remove]\n",
    "    for func in func_list:\n",
    "        df['CleanText'] = df['CleanText'].apply(func)\n",
    "\n",
    "    return df\n",
    "\n",
    "def featurize(df):\n",
    "    \n",
    "    df['NumWords'] = df['CleanText'].apply(get_num_words)\n",
    "    df['NumCharacters'] = df['CleanText'].apply(get_length)\n",
    "    df['AvgWordLength'] = df['CleanText'].apply(get_avg_wordlength)\n",
    "    df['NumSyllables'] = df['CleanText'].apply(get_num_syllables)\n",
    "    df['language_code'] = df['CleanText'].apply(detect_lang)\n",
    "    df = df.merge(language_codes,how='left',on='language_code')\n",
    "    df['Language'] = df['Language'].astype('str')\n",
    "    df['Language'] = df['Language'].apply(lambda x: x.lower().split(';')[0])\n",
    "    df['Language'] = df['Language'].apply(lambda x: x if x in languages else 'other')\n",
    "    df['Language'].fillna('None')\n",
    "    langdummies = pd.get_dummies(df['Language'],prefix='language_')\n",
    "    \n",
    "    df['CleanText_NoStop'] = df.apply(remove_stopwords,axis=1)\n",
    "    df['CleanText_NoStop_Stemmed'] = df.apply(stem_words,axis=1)\n",
    "    df.drop(['language_code','Language'],axis=1,inplace=True)\n",
    "    \n",
    "    tenses = df['CleanText'].apply(determine_tense)\n",
    "    tenses.columns = ['tense_' + col for col in tenses.columns]\n",
    "    \n",
    "    postagcounts = []\n",
    "    for index, row in df.iterrows():\n",
    "        postagcounts.append(get_pos_tags(row['CleanText']))    \n",
    "    postagdf = pd.DataFrame(postagcounts).fillna(0)\n",
    "    postagdf.index = df.index\n",
    "    postagdf.columns = ['postag_' + col for col in postagdf.columns]\n",
    "    \n",
    "    textblobsentdf = df['CleanText'].apply(textblobsent)\n",
    "    textblobsentdf.columns = ['TextblobPolarity','TextblobSubjectivity']\n",
    "    df = pd.concat([df,textblobsentdf,tenses,postagdf,langdummies],axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "annotations = clean_text(annotations)\n",
    "annotations = featurize(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df= .6,\n",
    "                                 min_df= .002, \n",
    "                                 stop_words=None,  \n",
    "                                 use_idf=True, \n",
    "                                 ngram_range=(1,4))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(featurized_df['CleanText_NoStop_Stemmed'].values.astype('U'))\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "tfidf_matrix = tfidf_matrix.todense()\n",
    "tfidf = pd.DataFrame(tfidf_matrix)\n",
    "tfidf.index = featurized_df.index\n",
    "tfidf.columns = terms\n",
    "print tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exclude = ['Source','Category','Topic','Annotation Text','CleanText','CleanText_NoStop','CleanText_NoStop_Stemmed',\n",
    "           'OCID','PDF Page Number','Article Reference','MD','VBP','VBZ','VBG','VBD','VBN','other',\"''\"]\n",
    "features = [col for col in annotations.columns.tolist() if not col in exclude]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.concat([tfidf,annotations[features]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.fillna(0,inplace=True)\n",
    "X = X.rename(columns = {'fit':'fit_feature'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_roc_auc(y_test, y_score, classes):\n",
    "    '''\n",
    "    Computes parameters needed for ROC Curve plot\n",
    "    '''\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    y4roc = pd.get_dummies(pd.DataFrame(y_test))\n",
    "    y_score_4roc = pd.get_dummies(pd.DataFrame(y_score))\n",
    "    y4roc.columns = classes\n",
    "    y_score_4roc.columns = classes\n",
    "    for col in y4roc.columns:\n",
    "        fpr[col], tpr[col], _ = roc_curve(y4roc[col], y_score_4roc[col])\n",
    "        roc_auc[col] = auc(fpr[col], tpr[col])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    ylist = [y4roc[col].tolist() for col in y4roc.columns]\n",
    "    ylist = [item for sublist in ylist for item in sublist]\n",
    "    yscorelist = [y_score_4roc[col].tolist() for col in y_score_4roc.columns]\n",
    "    yscorelist = [item for sublist in yscorelist for item in sublist]\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(ylist, yscorelist)\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    return fpr, tpr, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_roc(results, class_names):\n",
    "    fpr, tpr, roc_auc = [{} for i in range(3)]\n",
    "\n",
    "    lw = 1\n",
    "    roc_auc_all = {}\n",
    "\n",
    "    fpr = results['fpr'].iloc[0]\n",
    "    tpr = results['tpr'].iloc[0]\n",
    "    roc_auc = results['ovr_auc'].iloc[0]\n",
    "    n_classes = len(fpr) - 1\n",
    "\n",
    "    all_fpr = np.unique(np.concatenate([fpr[col] for col in class_names]))\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for col in class_names:\n",
    "        mean_tpr += interp(all_fpr, fpr[col], tpr[col])\n",
    "\n",
    "    # Average and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc_all[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    # Plot all ROC curves\n",
    "\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "             label='macro-average ROC curve (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc_all[\"macro\"]),\n",
    "             color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue','red','purple','blue'])\n",
    "    j = 0\n",
    "    for i, color in zip(class_names, colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "                 label='ROC curve of bin ' + class_names[j] + ' (area = {1:0.2f})'\n",
    "                 ''.format(i, roc_auc[i]))\n",
    "        j += 1\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Multiclass ROC Curves')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print str(n_folds) + ' Fold CV Score: ' + str(np.mean(cross_val_score(model, X, y, cv=n_folds)))\n",
    "model.fit(X_train, y_train)\n",
    "test_pred = model.predict(X_test)\n",
    "\n",
    "results = {}\n",
    "results['fpr'], results['tpr'], results['roc_auc'] = compute_roc_auc(y_test, test_pred, class_names)\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, test_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot confusion matrix\n",
    "sns.set_context(rc={\"figure.figsize\": (6, 6)})       \n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=model.classes_,\n",
    "                          title='Confusion matrix')\n",
    "\n",
    "#Plot ROC Curve\n",
    "plot_roc_curve(results, model.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_importances(clf,X):\n",
    "    '''\n",
    "    returns feature importances\n",
    "    '''\n",
    "    importances = clf.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    feat_importance = pd.DataFrame(X.columns, importances)    \n",
    "    feat_importance.reset_index(inplace=True)\n",
    "    feat_importance.columns = ['Importance','Feature']\n",
    "    feat_importance.sort_values(by='Importance', axis=0, ascending=True, inplace=True)\n",
    "    feat_importance.set_index('Feature',inplace=True)\n",
    "    return feat_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importances = {}\n",
    "importances = get_feature_importances(model,X)\n",
    "importances.index.name=None\n",
    "display(importances.sort_values(by='Importance',ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
