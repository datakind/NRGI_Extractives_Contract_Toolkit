{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](DataKind_orange.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classifier for contract clause types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, io, requests\n",
    "from collections import Counter\n",
    "from itertools import product\n",
    "import unicodedata\n",
    "import re, string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from textblob import TextBlob\n",
    "from langdetect import detect\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook', rc={'figure.figsize': (10, 6)}, font_scale=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy is used for Part of Speech tagging and Named Entity Recognition\n",
    "# spacy is a non-standard python library which can be installed using 'pip install spacy' from the command line\n",
    "# language models can be downloaded by running 'python -m spacy download <language>' from the command line\n",
    "import spacy\n",
    "supported_languages = ['en','fr','es']\n",
    "nlp_langs = {}\n",
    "for language in supported_languages:\n",
    "    nlp_langs[language]  = spacy.load(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Downloaded xls file annotations from resourcecontracts and openlandcontracts\n",
    "resource_folder = 'contract_data/Contracts_Annotations/resource_contracts/'\n",
    "land_folder = 'contract_data/Contracts_Annotations/openland_contracts/'\n",
    "# Most recently downloaded metadata from resourcecontracts.org/contracts\n",
    "rc_metadata = 'contract_data/resource_contract_2017-08-16.csv' \n",
    "# Most recently downloaded metadata from openlandcontracts.org/contracts\n",
    "ol_metadata = 'contract_data/openland_contract_2017-08-16.csv'\n",
    "metadata_files = [rc_metadata,ol_metadata]\n",
    "folders = [resource_folder,land_folder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = ['stabilization','royalties'] # list in priority order in case of duplicates\n",
    "n_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Languages must be supported by NLTK Snowball Stemmer and stopwords\n",
    "languages = ['english','french','spanish','italian','portuguese','dutch','swedish','german']\n",
    "s = requests.get('http://data.okfn.org/data/core/language-codes/r/language-codes.csv').content\n",
    "language_codes = pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    "language_codes.columns = ['language_code','DetectedLanguage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.DataFrame()\n",
    "for folder in folders:\n",
    "    xls_files = [f for f in os.listdir(folder) if f.lower().endswith('.xls')]\n",
    "    for xls in xls_files:\n",
    "        temp = pd.read_excel(folder + xls)\n",
    "        if len(temp) > 0:\n",
    "            temp['OCID'] = xls[:-4]\n",
    "            temp['Source'] = folder.split('/')[-2]\n",
    "            annotations = annotations.append(temp)\n",
    "print \"Number of annotations: \" + str(len(annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop blank and integer annotations and annotations less than 4 words\n",
    "annotations.dropna(subset=['Annotation Text'],inplace=True)\n",
    "annotations = annotations[annotations['Annotation Text'].apply(lambda x: type(x)!=int)].copy()\n",
    "annotations = annotations[annotations['Annotation Text'].apply(lambda x: len(x.split()) > 3)].copy()\n",
    "print len(annotations)\n",
    "# If duplicate text appears within the same contract, drop it\n",
    "annotations.drop_duplicates(['Annotation Text','OCID','Category'],inplace=True)\n",
    "print len(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = [label.lower() for label in labels]\n",
    "annotations['label'] = [x.lower() if x.lower() in labels else 'other' for x in annotations['Category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given some duplicate text with different Category labels, keep the labels that are in our target label list when dropping\n",
    "sort_num = range(len(labels) + 1)\n",
    "sort_key = dict(zip(labels,sort_num))\n",
    "sort_key['other'] = sort_num[-1]\n",
    "annotations['sort_key'] = [sort_key[x] for x in annotations['label']]\n",
    "annotations.sort_values(by='sort_key',inplace=True,ascending=True)\n",
    "annotations.drop_duplicates(['Annotation Text','OCID'],keep='first',inplace=True)\n",
    "print len(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join metadata from contracts repository\n",
    "metadata = pd.DataFrame()\n",
    "for filename in metadata_files:\n",
    "    temp = pd.read_csv(filename)\n",
    "    metadata = metadata.append(temp)\n",
    "    \n",
    "annotations = annotations.merge(metadata[['OCID','Language','Country Name','Resource','Contract Type','Document Type']],how='left',on='OCID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = list(annotations['label'])\n",
    "yunique = list(np.unique(y))\n",
    "for item in yunique:\n",
    "    print str(\"{0:.2f}%\".format(100*y.count(item) / float(len(y)))) + \" \" + item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "annotations.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punctuation_remove(text):\n",
    "    \"\"\"\n",
    "    Mutates and returns text where all punctuation besides underscores,\n",
    "    are replaced\n",
    "    \"\"\"\n",
    "    punctuation_re = r'[^\\w\\s_]'\n",
    "    new_text = nltk.regexp.re.sub(punctuation_re, ' ', text)\n",
    "    return new_text\n",
    "\n",
    "def replace_numbers(text):\n",
    "    ''' \n",
    "    Removes all characters but periods, commas and alpha-numeric and \n",
    "    returns all numeric values replace with the word numeric_value\n",
    "    '''\n",
    "#     allowed = {\",\", \".\",\" \",\"%\"}.union(string.ascii_letters).union([str(num) for num in range(0,10)])\n",
    "#     filtered = ''.join([character for character in text if character in allowed])\n",
    "    wordlist = text.split()\n",
    "    for i in range(len(wordlist)):\n",
    "        if '$' in wordlist[i]:\n",
    "            try:\n",
    "                int(wordlist[i].split('$')[-1].replace(',','').replace('.','').replace('-','').replace(')','').replace('(',''))\n",
    "                wordlist[i] = ' '.join(wordlist[i].split('$')[:-1]) + ' dollar_value'\n",
    "            except:\n",
    "                pass\n",
    "        elif u'\\xb0' in wordlist[i]:\n",
    "            wordlist[i] == 'degree_value'\n",
    "        elif '%' in wordlist[i]:\n",
    "            try: \n",
    "                int(wordlist[i].replace(',','').replace('.','').replace('-','').replace('/','').replace('%','').replace(')','').replace('(',''))\n",
    "                wordlist[i] = 'percent_value'\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            try: \n",
    "                int(wordlist[i].replace(',','').replace('.','').replace('-','').replace('/','').replace(')','').replace('(',''))\n",
    "                wordlist[i] = 'numeric_value'\n",
    "            except:\n",
    "                pass\n",
    "    return ' '.join(wordlist)\n",
    "\n",
    "def perform_lowercase(text):\n",
    "    \"\"\"\n",
    "    Mutates and returns text where all characters are lowercased\n",
    "    \"\"\"\n",
    "    try:\n",
    "        new_text = text.lower()\n",
    "    except:\n",
    "        new_text = str(text).lower()\n",
    "    return new_text\n",
    "\n",
    "def underscore_remove(text):\n",
    "    '''\n",
    "    replaces multiple underscores with text fillintheblank\n",
    "    and single underscore with space\n",
    "    '''\n",
    "    double_underscore_re = r'(__[a-zA-Z0-9_]*(__)?)'\n",
    "    text = nltk.regexp.re.sub(double_underscore_re,'fillintheblank',text)\n",
    "    return text.replace('_',' ')\n",
    "\n",
    "def doublespace_remove(text):\n",
    "    return re.sub(' +',' ',text)\n",
    "\n",
    "def textblobsent(text):\n",
    "    '''\n",
    "    returns the TextBlob polarity and subjectivity\n",
    "    '''\n",
    "    text = text.encode('ascii','replace')\n",
    "    sent = TextBlob(text).sentiment\n",
    "    return pd.Series([sent.polarity,sent.subjectivity])\n",
    "\n",
    "def get_avg_wordlength(document):\n",
    "    wordlengths = [len(word) for word in document.split()]\n",
    "    if len(wordlengths) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.mean(wordlengths)\n",
    "\n",
    "def multilingual_pos(text,language):\n",
    "    if language in nlp_langs.keys():\n",
    "        try:\n",
    "            tokens = nlp_langs[language](text)\n",
    "            tags = [token.pos_ for token in tokens]\n",
    "            counts = Counter(tags).items()\n",
    "            countdict = {}\n",
    "            for key, value in counts:\n",
    "                countdict[key] = value\n",
    "            return countdict\n",
    "        except:\n",
    "            print type(text), text\n",
    "    else:\n",
    "        return {'unsupported_language':1}\n",
    "    \n",
    "def detect_lang(text):\n",
    "    '''\n",
    "    Returns detected language\n",
    "    '''\n",
    "    text = doublespace_remove(text)\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return 'None'\n",
    "    \n",
    "def remove_stopwords(row):\n",
    "    '''\n",
    "    Multilingual stopwords removal\n",
    "    '''\n",
    "    language = row['Language']\n",
    "    if language in languages:\n",
    "        text = ' '.join([word for word in row['CleanText'].split(' ') if word not in stopwords.words(language)])\n",
    "        return text\n",
    "    else:\n",
    "        return row['CleanText']\n",
    "    \n",
    "def stem_words(row):\n",
    "    ''' \n",
    "    Multilingual word stemmer\n",
    "    '''\n",
    "    language = row['Language']    \n",
    "    if language in languages:\n",
    "        stemmer = SnowballStemmer(language)\n",
    "        text = ' '.join([stemmer.stem(word) for word in row['CleanText_NoStop'].split(' ')])\n",
    "        return text\n",
    "    else:\n",
    "        return row['CleanText_NoStop']\n",
    "    \n",
    "def clean_metadata(text):\n",
    "    if type(text) in [float,int]:\n",
    "        return text\n",
    "    elif type(text) == str:\n",
    "        return text.lower().split(';')[0]\n",
    "    else:\n",
    "        text = text.encode('ascii','replace')\n",
    "        return text.lower().split(';')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(df):\n",
    "    df['Annotation Text'].fillna('',inplace=True)\n",
    "    df['CleanText'] = df['Annotation Text']\n",
    "    func_list = [perform_lowercase,replace_numbers,punctuation_remove,underscore_remove, doublespace_remove]\n",
    "    for func in func_list:\n",
    "        df['CleanText'] = df['CleanText'].apply(func)\n",
    "\n",
    "    return df\n",
    "\n",
    "def featurize(df):\n",
    "    \n",
    "    df['AvgWordLength'] = df['CleanText'].apply(get_avg_wordlength)\n",
    "    df['language_code'] = df['CleanText'].apply(detect_lang)\n",
    "    df = df.merge(language_codes,how='left',on='language_code')\n",
    "    df['DetectedLanguage'] = df['DetectedLanguage'].astype('str')\n",
    "    df['DetectedLanguage'] = df['DetectedLanguage'].apply(lambda x: x.lower().split(';')[0])\n",
    "    df['DetectedLanguage'] = df['DetectedLanguage'].apply(lambda x: x if x in languages else 'other')\n",
    "    df['DetectedLanguage'].fillna('None')\n",
    "    langdummies = pd.get_dummies(df['DetectedLanguage'],prefix='detected_language')\n",
    "    # Drop least frequent language\n",
    "    langdummies.drop([col for col, val in langdummies.sum().iteritems() if val == langdummies.sum().min()],axis=1,inplace=True)\n",
    "    \n",
    "    df['CleanText_NoStop'] = df.apply(remove_stopwords,axis=1)\n",
    "    df['CleanText_NoStop_Stemmed'] = df.apply(stem_words,axis=1)\n",
    "    df.drop(['language_code','DetectedLanguage'],axis=1,inplace=True)\n",
    "    \n",
    "    postagcounts = []\n",
    "    for index, row in df.iterrows():\n",
    "        postagcounts.append(multilingual_pos(row['Annotation Text'],row['Language']))    \n",
    "    postagdf = pd.DataFrame(postagcounts).fillna(0)\n",
    "    postagdf.index = df.index\n",
    "    postagdf.columns = ['postag_' + col for col in postagdf.columns]\n",
    "\n",
    "    # create dummy variables for categoricals\n",
    "    df['Resource'] = df['Resource'].apply(clean_metadata)\n",
    "    df['Contract Type'] = df['Contract Type'].apply(clean_metadata)\n",
    "    df['Document Type'] = df['Document Type'].apply(lambda x: x.lower().split(';')[0])\n",
    "    dummy_cols = ['Language','Country Name','Resource','Contract Type','Document Type']\n",
    "    dummies = pd.get_dummies(df[dummy_cols],prefix = dummy_cols)\n",
    "    # drop lowest least frequent dummy columns for each\n",
    "    for dummy_col in dummy_cols:\n",
    "        cols = [col for col in dummies.columns if col.startswith(dummy_col)]\n",
    "        dummies.drop([col for col, val in dummies[cols].sum().iteritems() if val == dummies[cols].sum().min()],axis=1,inplace=True)\n",
    "    df.drop(dummy_cols, axis=1,inplace=True)\n",
    "        \n",
    "    textblobsentdf = df['CleanText'].apply(textblobsent)\n",
    "    textblobsentdf.columns = ['TextblobPolarity','TextblobSubjectivity']\n",
    "    df = pd.concat([df,textblobsentdf,langdummies,postagdf,dummies],axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = clean_text(annotations)\n",
    "annotations = featurize(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df= .6,\n",
    "                                 min_df= .001, \n",
    "                                 stop_words=None,  \n",
    "                                 use_idf=True, \n",
    "                                 ngram_range=(1,4))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(annotations['CleanText_NoStop_Stemmed'].values.astype('U'))\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "tfidf_matrix = tfidf_matrix.todense()\n",
    "tfidf = pd.DataFrame(tfidf_matrix)\n",
    "tfidf.index = annotations.index\n",
    "tfidf.columns = terms\n",
    "# tfidf.columns = [term.encode('ascii','replace') for term in tfidf.columns]\n",
    "print tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exclude = ['Source','Category','Topic','Annotation Text','CleanText','CleanText_NoStop','CleanText_NoStop_Stemmed',\n",
    "           'OCID','PDF Page Number','Article Reference','MD','VBP','VBZ','VBG','VBD','VBN','other',\"''\",'label','sort_key']\n",
    "features = [str(col) for col in annotations.columns.tolist() if not col in exclude]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_terms = []\n",
    "for term in terms:\n",
    "    for word in term.split():\n",
    "        try:\n",
    "            int(word)\n",
    "            if not term in numeric_terms:\n",
    "                numeric_terms.append(term)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 10                           183\n",
    "# sec 11                       188\n",
    "# 000                          232\n",
    "# 11, 288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf[numeric_terms].apply(lambda column: (column != 0).sum()).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print len(numeric_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_numbers('MOPP will invest US$64,000,000,000. US$48,000,000,000 will be for the development of the concession area, and US$16,000,000,000 will be for the outgrowers program. The investment for the concession area will be divided as follows: initial investment of US$15,000,000,000 during the first numeric_value years (which includes investments prior to date the contract comes into force), second investment of US$ numeric_value over a second numeric_value year term, and a final US$ numeric_value during the rest of the term of the contract.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = annotations[['OCID','Category','Annotation Text','CleanText']].iloc[8473]\n",
    "print test[['OCID','Category']]\n",
    "print test['CleanText']\n",
    "print '\\n'\n",
    "print replace_numbers(test['Annotation Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf[tfidf['000 000'] > 0]['000 000'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([tfidf,annotations[features]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.fillna(0,inplace=True)\n",
    "X = X.rename(columns = {'fit':'fit_feature'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = OneVsRestClassifier(RandomForestClassifier(\n",
    "                                                    n_estimators=500,\n",
    "                                                    max_features=.2, \n",
    "                                                    min_samples_leaf=50,\n",
    "                                                    class_weight='balanced_subsample',\n",
    "                                                    min_samples_split=2,\n",
    "                                                    bootstrap=True,\n",
    "                                                    max_depth=None, \n",
    "                                                    n_jobs=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print str(n_folds) + ' Fold CV Accuracy Score: ' + str(np.mean(cross_val_score(model, X, y, cv=n_folds)))\n",
    "model.fit(X_train, y_train)\n",
    "test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Test Set Classification Report \\n\" + classification_report(y_test, test_pred, target_names=yunique)\n",
    "print \"Test Set Accuracy: \" + str(accuracy_score(y_test, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(y_test, test_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot confusion matrix\n",
    "sns.set_context(rc={\"figure.figsize\": (8, 8)})       \n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=model.classes_,\n",
    "                          title='Confusion matrix ');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_importances(clf,X):\n",
    "    '''\n",
    "    returns feature importances\n",
    "    '''\n",
    "    classes = clf.classes_\n",
    "    feat_importance = {}\n",
    "    for i in range(len(classes)):\n",
    "        importances = clf.estimators_[i].feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        feat_importance[classes[i]] = pd.DataFrame(X.columns, importances)    \n",
    "        feat_importance[classes[i]].reset_index(inplace=True)\n",
    "        feat_importance[classes[i]].columns = ['Importance','Feature']\n",
    "        feat_importance[classes[i]].sort_values(by='Importance', axis=0, ascending=True, inplace=True)\n",
    "        feat_importance[classes[i]].set_index('Feature',inplace=True)\n",
    "    return feat_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "sns.set_context(rc={\"figure.figsize\": (8, 6)})       \n",
    "importances = {}\n",
    "importances = get_feature_importances(model,X)\n",
    "for i in model.classes_[1:]:\n",
    "    print 'Top ' + str(n) + ' Features for ' + i\n",
    "    importances[i].index.name=None \n",
    "    importances[i].sort_values(by='Importance',ascending=False).head(n).plot(kind='barh',title=i.title() + ' Top ' + str(n) + ' Features',legend=False, color='lightblue')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "annotations['Actual'] = y\n",
    "annotations['Predicted'] = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed = len(annotations[annotations['Predicted'] != annotations['Actual']])\n",
    "hit = len(annotations) - missed\n",
    "print 'Hit: ' + str(hit)\n",
    "print 'Missed: ' + str(missed)\n",
    "print 'Accuracy: ' + str(round(hit / float(len(annotations)),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positives = annotations[(annotations['Predicted'] != annotations['Actual']) \n",
    "                              & (annotations['Predicted'] != 'other')]\n",
    "false_negatives = annotations[(annotations['Predicted'] != annotations['Actual']) \n",
    "                              & (annotations['Predicted'] == 'other')]\n",
    "true_positives = annotations[(annotations['Predicted'] == annotations['Actual']) \n",
    "                              & (annotations['Predicted'] != 'other')]\n",
    "print 'False Positives: ' + str(len(false_positives))\n",
    "print 'False Negatives: ' + str(len(false_negatives))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positives[false_  positives['language_english'] == 1][['OCID','Category','Predicted','CleanText']].to_csv('classification_results/False_Positives.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "false_negatives[['OCID','Category','Predicted','CleanText']].to_csv('classification_results/False_Negatives.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
