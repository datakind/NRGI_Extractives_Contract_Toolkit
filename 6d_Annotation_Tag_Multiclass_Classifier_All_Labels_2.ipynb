{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](DataKind_orange.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classifier for contract clause types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, io\n",
    "from collections import Counter\n",
    "from itertools import product\n",
    "import re, string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, cross_validate\n",
    "from sklearn.metrics import accuracy_score, classification_report, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from textblob import TextBlob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook', rc={'figure.figsize': (10, 6)}, font_scale=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_outfile = 'models/model_23_classes.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# spacy is used for Part of Speech tagging and Named Entity Recognition\n",
    "# spacy is a non-standard python library which can be installed using 'pip install spacy' from the command line\n",
    "# language models can be downloaded by running 'python -m spacy download <language>' from the command line\n",
    "import spacy\n",
    "supported_languages = ['en','fr','es']\n",
    "language_dict = {'en':'english','fr':'french','es':'spanish'}\n",
    "nlp_langs = {}\n",
    "for language in supported_languages:\n",
    "    nlp_langs[language]  = spacy.load(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Downloaded xls file annotations from resourcecontracts and openlandcontracts\n",
    "resource_folder = 'contract_data/Contracts_Annotations/resource_contracts/'\n",
    "land_folder = 'contract_data/Contracts_Annotations/openland_contracts/'\n",
    "# Most recently downloaded metadata from resourcecontracts.org/contracts\n",
    "rc_metadata = 'contract_data/resource_contract_2017-08-16.csv' \n",
    "# Most recently downloaded metadata from openlandcontracts.org/contracts\n",
    "ol_metadata = 'contract_data/openland_contract_2017-08-16.csv'\n",
    "# CSV with any incorrect labels to fix\n",
    "label_fixes = 'contract_data/label_fixes/label_fixes.csv'\n",
    "\n",
    "metadata_files = [rc_metadata,ol_metadata]\n",
    "folders = [resource_folder,land_folder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "general_labels = {'environmental protections':'environmental provisions',\n",
    "'environmental impact assessment and management plan':'environmental provisions',\n",
    "'other - financial/fiscal':'fiscal provisions',\n",
    "'royalties':'fiscal provisions',\n",
    "'bonuses':'fiscal provisions',\n",
    "'income tax: rate':'fiscal provisions',\n",
    "'income tax: exemptions':'fiscal provisions',\n",
    "'income tax: other':'fiscal provisions',\n",
    "'withholding tax':'fiscal provisions',\n",
    "'capital gains tax':'fiscal provisions',\n",
    "'state participation':'fiscal provisions',\n",
    "'financial obligations - community or commodity funds':'social provisions',\n",
    "'community consultation':'social provisions',\n",
    "'local employment':'social provisions',\n",
    "'local procurement':'social provisions',\n",
    "'local development agreement':'social provisions',\n",
    "'training':'social provisions',\n",
    "'infrastructure':'operations',\n",
    "'infrastructure - third party use':'operations',\n",
    "'work and investment commitments':'operations',\n",
    "'confidentiality':'legal rules',\n",
    "'stabilization':'legal rules',\n",
    "'arbitration and dispute resolution':'legal rules'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['income tax: other', 'stabilization', 'confidentiality', 'royalties', 'environmental protections', 'capital gains tax', 'local procurement', 'bonuses', 'income tax: exemptions', 'local development agreement', 'environmental impact assessment and management plan', 'infrastructure', 'arbitration and dispute resolution', 'work and investment commitments', 'community consultation', 'income tax: rate', 'local employment', 'training', 'state participation', 'financial obligations - community or commodity funds', 'infrastructure - third party use', 'withholding tax', 'other - financial/fiscal']\n"
     ]
    }
   ],
   "source": [
    "n_folds = 5\n",
    "labels = list(set(general_labels.keys()))\n",
    "print labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of annotations: 20609\n"
     ]
    }
   ],
   "source": [
    "annotations = pd.DataFrame()\n",
    "for folder in folders:\n",
    "    xls_files = [f for f in os.listdir(folder) if f.lower().endswith('.xls')]\n",
    "    for xls in xls_files:\n",
    "        temp = pd.read_excel(folder + xls)\n",
    "        if len(temp) > 0:\n",
    "            temp['OCID'] = xls[:-4]\n",
    "            temp['Source'] = folder.split('/')[-2]\n",
    "            annotations = annotations.append(temp)\n",
    "annotations['Category'] = annotations['Category'].str.lower()\n",
    "print \"Number of annotations: \" + str(len(annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17885\n",
      "13795\n"
     ]
    }
   ],
   "source": [
    "# drop blank and integer annotations and annotations less than 4 words\n",
    "annotations.dropna(subset=['Annotation Text'],inplace=True)\n",
    "annotations = annotations[annotations['Annotation Text'].apply(lambda x: type(x)!=int)].copy()\n",
    "annotations = annotations[annotations['Annotation Text'].apply(lambda x: len(x.split()) > 3)].copy()\n",
    "print len(annotations)\n",
    "# If duplicate text appears within the same contract, drop it\n",
    "annotations.drop_duplicates(['Annotation Text','OCID','Category'],inplace=True)\n",
    "print len(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "annotations['General Category'] = annotations['Category'].replace(general_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = [label.lower() for label in labels]\n",
    "annotations['label'] = [x.lower() if x.lower() in labels else 'other' for x in annotations['Category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13710\n"
     ]
    }
   ],
   "source": [
    "# Given some duplicate text with different Category labels, keep the labels that are in our target label list when dropping\n",
    "sort_num = range(len(labels) + 1)\n",
    "sort_key = dict(zip(labels,sort_num))\n",
    "sort_key['other'] = sort_num[-1]\n",
    "annotations['sort_key'] = [sort_key[x] for x in annotations['label']]\n",
    "annotations.sort_values(by='sort_key',inplace=True,ascending=True)\n",
    "annotations.drop_duplicates(['Annotation Text','OCID'],keep='first',inplace=True)\n",
    "print len(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Join metadata from contracts repository\n",
    "metadata = pd.DataFrame()\n",
    "for filename in metadata_files:\n",
    "    temp = pd.read_csv(filename)\n",
    "    metadata = metadata.append(temp)\n",
    "    \n",
    "annotations = annotations.merge(metadata[['OCID','Language','Country Name','Resource','Contract Type','Document Type']],how='left',on='OCID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.99% arbitration and dispute resolution\n",
      "1.26% bonuses\n",
      "0.18% capital gains tax\n",
      "2.66% confidentiality\n",
      "2.31% environmental impact assessment and management plan\n",
      "2.80% environmental protections\n",
      "0.57% financial obligations - community or commodity funds\n",
      "1.71% income tax: exemptions\n",
      "1.46% income tax: other\n",
      "1.79% income tax: rate\n",
      "2.82% infrastructure\n",
      "1.22% infrastructure - third party use\n",
      "1.12% local development agreement\n",
      "2.82% local employment\n",
      "2.01% local procurement\n",
      "58.08% other\n",
      "1.90% other - financial/fiscal\n",
      "2.44% royalties\n",
      "1.80% stabilization\n",
      "1.39% state participation\n",
      "1.85% training\n",
      "0.28% withholding tax\n",
      "3.52% work and investment commitments\n"
     ]
    }
   ],
   "source": [
    "y = list(annotations['label'])\n",
    "yunique = list(np.unique(y))\n",
    "for item in yunique:\n",
    "    print str(\"{0:.2f}%\".format(100*y.count(item) / float(len(y)))) + \" \" + item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Annotation Text</th>\n",
       "      <th>PDF Page Number</th>\n",
       "      <th>Article Reference</th>\n",
       "      <th>OCID</th>\n",
       "      <th>Source</th>\n",
       "      <th>General Category</th>\n",
       "      <th>label</th>\n",
       "      <th>sort_key</th>\n",
       "      <th>Language</th>\n",
       "      <th>Country Name</th>\n",
       "      <th>Resource</th>\n",
       "      <th>Contract Type</th>\n",
       "      <th>Document Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>income tax: other</td>\n",
       "      <td>Fiscal</td>\n",
       "      <td>Cavalla Rubber Corporation shall be entitled t...</td>\n",
       "      <td>38</td>\n",
       "      <td>Arts.  4.4 (b), 9.4, 19.4, 19.10, 19.19, 19.22...</td>\n",
       "      <td>ocds-591adf-6994803993</td>\n",
       "      <td>openland_contracts</td>\n",
       "      <td>fiscal provisions</td>\n",
       "      <td>income tax: other</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>Liberia</td>\n",
       "      <td>Oil palm or palm oils;Oil palm products;Rubber...</td>\n",
       "      <td>Concession Agreement</td>\n",
       "      <td>Contract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>income tax: other</td>\n",
       "      <td>Fiscal</td>\n",
       "      <td>Etap et Buttes  se doivent de payer les taxes ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Art.3.2</td>\n",
       "      <td>ocds-591adf-4484816003</td>\n",
       "      <td>resource_contracts</td>\n",
       "      <td>fiscal provisions</td>\n",
       "      <td>income tax: other</td>\n",
       "      <td>0</td>\n",
       "      <td>fr</td>\n",
       "      <td>Tunisia</td>\n",
       "      <td>Hydrocarbons</td>\n",
       "      <td>Concession Agreement</td>\n",
       "      <td>Contract</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Category   Topic  \\\n",
       "0  income tax: other  Fiscal   \n",
       "1  income tax: other  Fiscal   \n",
       "\n",
       "                                     Annotation Text  PDF Page Number  \\\n",
       "0  Cavalla Rubber Corporation shall be entitled t...               38   \n",
       "1  Etap et Buttes  se doivent de payer les taxes ...                5   \n",
       "\n",
       "                                   Article Reference                    OCID  \\\n",
       "0  Arts.  4.4 (b), 9.4, 19.4, 19.10, 19.19, 19.22...  ocds-591adf-6994803993   \n",
       "1                                            Art.3.2  ocds-591adf-4484816003   \n",
       "\n",
       "               Source   General Category              label  sort_key  \\\n",
       "0  openland_contracts  fiscal provisions  income tax: other         0   \n",
       "1  resource_contracts  fiscal provisions  income tax: other         0   \n",
       "\n",
       "  Language Country Name                                           Resource  \\\n",
       "0       en      Liberia  Oil palm or palm oils;Oil palm products;Rubber...   \n",
       "1       fr      Tunisia                                       Hydrocarbons   \n",
       "\n",
       "          Contract Type Document Type  \n",
       "0  Concession Agreement      Contract  \n",
       "1  Concession Agreement      Contract  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def punctuation_remove(text):\n",
    "    \"\"\"\n",
    "    Mutates and returns text where all punctuation besides underscores,\n",
    "    are replaced\n",
    "    \"\"\"\n",
    "    punctuation_re = r'[^\\w\\s_]'\n",
    "    new_text = nltk.regexp.re.sub(punctuation_re, ' ', text)\n",
    "    return new_text\n",
    "\n",
    "def replace_numbers(text):\n",
    "    ''' \n",
    "    Removes all characters but periods, commas and alpha-numeric and \n",
    "    returns all numeric values replace with the word numeric_value\n",
    "    '''\n",
    "#     allowed = {\",\", \".\",\" \",\"%\"}.union(string.ascii_letters).union([str(num) for num in range(0,10)])\n",
    "#     filtered = ''.join([character for character in text if character in allowed])\n",
    "    wordlist = text.split()\n",
    "    for i in range(len(wordlist)):\n",
    "        if '$' in wordlist[i]:\n",
    "            try:\n",
    "                int(wordlist[i].split('$')[-1].replace(',','').replace('.','').replace('-','').replace(')','').replace('(','').replace('\\'','').replace(';','').replace(':',''))\n",
    "                wordlist[i] = ' '.join(wordlist[i].split('$')[:-1]) + ' dollarvalue'\n",
    "            except:\n",
    "                pass\n",
    "        elif u'\\xb0' in wordlist[i]:\n",
    "            wordlist[i] == 'degreevalue'\n",
    "        elif '%' in wordlist[i]:\n",
    "            try: \n",
    "                int(wordlist[i].replace(',','').replace('.','').replace('-','').replace('/','').replace('%','').replace(')','').replace('(','').replace('\\'','').replace(';','').replace(':',''))\n",
    "                wordlist[i] = 'percentvalue'\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            try: \n",
    "                int(wordlist[i].replace(',','').replace('.','').replace('-','').replace('/','').replace(')','').replace('(','').replace('\\'','').replace(';','').replace(':',''))\n",
    "                wordlist[i] = 'numericvalue'\n",
    "            except:\n",
    "                pass\n",
    "    return ' '.join(wordlist)\n",
    "\n",
    "def perform_lowercase(text):\n",
    "    \"\"\"\n",
    "    Mutates and returns text where all characters are lowercased\n",
    "    \"\"\"\n",
    "    try:\n",
    "        new_text = text.lower()\n",
    "    except:\n",
    "        new_text = str(text).lower()\n",
    "    return new_text\n",
    "\n",
    "def underscore_remove(text):\n",
    "    '''\n",
    "    replaces multiple underscores with text fillintheblank\n",
    "    and single underscore with space\n",
    "    '''\n",
    "    double_underscore_re = r'(__[a-zA-Z0-9_]*(__)?)'\n",
    "    text = nltk.regexp.re.sub(double_underscore_re,'fillintheblank',text)\n",
    "    return text.replace('_',' ')\n",
    "\n",
    "def doublespace_remove(text):\n",
    "    return re.sub(' +',' ',text)\n",
    "\n",
    "def textblobsent(text):\n",
    "    '''\n",
    "    returns the TextBlob polarity and subjectivity\n",
    "    '''\n",
    "    text = text.encode('ascii','replace')\n",
    "    sent = TextBlob(text).sentiment\n",
    "    return pd.Series([sent.polarity,sent.subjectivity])\n",
    "\n",
    "def get_avg_wordlength(document):\n",
    "    wordlengths = [len(word) for word in document.split()]\n",
    "    if len(wordlengths) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.mean(wordlengths)\n",
    "\n",
    "def get_multilingual_pos(text,language):\n",
    "    if language in nlp_langs.keys():\n",
    "        try:\n",
    "            tokens = nlp_langs[language](text)\n",
    "            tags = [token.pos_ for token in tokens]\n",
    "            counts = Counter(tags).items()\n",
    "            countdict = {}\n",
    "            for key, value in counts:\n",
    "                countdict[key] = value\n",
    "            return countdict\n",
    "        except:\n",
    "            return {'pos_error':1}\n",
    "    else:\n",
    "        return {'unsupported_language':1}\n",
    "\n",
    "def get_multilingual_entities(row):\n",
    "    \n",
    "    text = row['CleanText']\n",
    "    language = row['Language']\n",
    "    if language in nlp_langs.keys():\n",
    "        try:\n",
    "            doc = nlp_langs[language](text)\n",
    "            labels = [ent.label_ for ent in doc.ents]\n",
    "            texts = [ent.text for ent in doc.ents]\n",
    "            starts = [ent.start_char for ent in doc.ents]\n",
    "            ends = [ent.end_char for ent in doc.ents]\n",
    "            textlens = [len(word) for word in texts]\n",
    "            labellens = [len(label) for label in labels]\n",
    "            diffs = [textlens[i] - labellens[i] for i in range(len(textlens))]\n",
    "            diffsum = [sum(diffs[0:i]) for i in range(len(diffs))]\n",
    "\n",
    "            for i in range(len(labels)):\n",
    "                text = text[0:starts[i] - diffsum[i]] + 'entity' + labels[i] + text[ends[i] - diffsum[i]:]\n",
    "            return text\n",
    "        except:\n",
    "            return text\n",
    "    else:\n",
    "        return text\n",
    "    \n",
    "def remove_stopwords(row):\n",
    "    '''\n",
    "    Multilingual stopwords removal\n",
    "    '''\n",
    "    try:\n",
    "        language = language_dict[row['Language']]\n",
    "        text = ' '.join([word for word in row['CleanText'].split(' ') if word not in stopwords.words(language)])\n",
    "        return text\n",
    "    except:\n",
    "        return row['CleanText']\n",
    "    \n",
    "def stem_words(row):\n",
    "    ''' \n",
    "    Multilingual word stemmer\n",
    "    '''\n",
    "    language = language_dict[row['Language']]\n",
    "    try:\n",
    "        stemmer = SnowballStemmer(language)\n",
    "        text = ' '.join([stemmer.stem(word) for word in row['CleanText_NoStop'].split(' ')])\n",
    "        return text\n",
    "    except:\n",
    "        return row['CleanText_NoStop']\n",
    "    \n",
    "def clean_metadata(text):\n",
    "    if type(text) in [float,int]:\n",
    "        return text\n",
    "    elif type(text) == str:\n",
    "        return text.lower().split(';')[0]\n",
    "    else:\n",
    "        text = text.encode('ascii','replace')\n",
    "        return text.lower().split(';')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(df):\n",
    "    df['Annotation Text'].fillna('',inplace=True)\n",
    "    df['CleanText'] = df['Annotation Text']\n",
    "    df['CleanText'] = df.apply(get_multilingual_entities,axis=1)\n",
    "    func_list = [perform_lowercase,replace_numbers,punctuation_remove,underscore_remove, doublespace_remove]\n",
    "    for func in func_list:\n",
    "        df['CleanText'] = df['CleanText'].apply(func)\n",
    "\n",
    "    return df\n",
    "\n",
    "def featurize(df):\n",
    "    \n",
    "    df['AvgWordLength'] = df['CleanText'].apply(get_avg_wordlength)\n",
    "    \n",
    "    df['CleanText_NoStop'] = df.apply(remove_stopwords,axis=1)\n",
    "    df['CleanText_NoStop_Stemmed'] = df.apply(stem_words,axis=1)\n",
    "    \n",
    "    postagcounts = []\n",
    "    entitycounts = []\n",
    "    for index, row in df.iterrows():\n",
    "        postagcounts.append(get_multilingual_pos(row['Annotation Text'],row['Language']))    \n",
    "    postagdf = pd.DataFrame(postagcounts).fillna(0)\n",
    "    postagdf.index = df.index\n",
    "    postagdf.columns = ['postag_' + col for col in postagdf.columns]\n",
    "    \n",
    "    # create dummy variables for categoricals\n",
    "    df['Resource'] = df['Resource'].apply(clean_metadata)\n",
    "    df['Contract Type'] = df['Contract Type'].apply(clean_metadata)\n",
    "    df['Document Type'] = df['Document Type'].apply(lambda x: x.lower().split(';')[0])\n",
    "    dummy_cols = ['Language','Country Name','Resource','Contract Type','Document Type']\n",
    "    dummies = pd.get_dummies(df[dummy_cols],prefix = dummy_cols)\n",
    "    # drop lowest least frequent dummy columns for each\n",
    "    for dummy_col in dummy_cols:\n",
    "        cols = [col for col in dummies.columns if col.startswith(dummy_col)]\n",
    "        dummies.drop([col for col, val in dummies[cols].sum().iteritems() if val == dummies[cols].sum().min()],axis=1,inplace=True)\n",
    "    df.drop(dummy_cols, axis=1,inplace=True)\n",
    "        \n",
    "    textblobsentdf = df['CleanText'].apply(textblobsent)\n",
    "    textblobsentdf.columns = ['TextblobPolarity','TextblobSubjectivity']\n",
    "    df = pd.concat([df,textblobsentdf,postagdf,dummies],axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "annotations = clean_text(annotations)\n",
    "annotations = featurize(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df= .6,\n",
    "                                 min_df= .002, \n",
    "                                 stop_words=None,  \n",
    "                                 use_idf=True, \n",
    "                                 ngram_range=(1,5))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(annotations['CleanText_NoStop_Stemmed'].values.astype('U'))\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "tfidf_matrix = tfidf_matrix.todense()\n",
    "tfidf = pd.DataFrame(tfidf_matrix)\n",
    "tfidf.index = annotations.index\n",
    "tfidf.columns = terms\n",
    "print tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exclude = ['Source','Category','General Category','Topic','Annotation Text','CleanText','CleanText_NoStop','CleanText_NoStop_Stemmed',\n",
    "           'OCID','PDF Page Number','Article Reference','MD','VBP','VBZ','VBG','VBD','VBN','other',\"''\",'label','sort_key']\n",
    "features = [str(col) for col in annotations.columns.tolist() if not col in exclude]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.concat([tfidf,annotations[features]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.fillna(0,inplace=True)\n",
    "X = X.rename(columns = {'fit':'fit_feature'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = OneVsRestClassifier(RandomForestClassifier(\n",
    "                                                    n_estimators=500,\n",
    "                                                    max_features=.33, \n",
    "                                                    n_jobs=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scoring = {'acc': 'accuracy',\n",
    "           'rec_macro': 'recall_macro'}\n",
    "scores = cross_validate(model, X, y, scoring=scoring,\n",
    "                         cv=n_folds, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print str(n_folds) + \" Fold Accuracy: \" + str(np.mean(scores['test_acc']))\n",
    "print str(n_folds) + \" Fold Recall: \" + str(np.mean(scores['test_rec_macro']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "# print str(n_folds) + ' Fold CV Accuracy Score: ' + str(np.mean(cross_val_score(model, X, y, cv=n_folds)))\n",
    "# model.fit(X_train, y_train)\n",
    "# test_pred = model.predict(X_test)\n",
    "# print \"Test Set Classification Report \\n\" + classification_report(y_test, test_pred, target_names=yunique)\n",
    "# print \"Test Set Accuracy: \" + str(accuracy_score(y_test, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joblib.dump(model, model_outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = cross_val_predict(model,X,y,cv=n_folds)\n",
    "cnf_matrix = confusion_matrix(y, y_pred)\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "sns.set_context(rc={\"figure.figsize\": (8, 8)})       \n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=model.classes_,\n",
    "                          title='Confusion matrix');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_importances(clf,X):\n",
    "    '''\n",
    "    returns feature importances\n",
    "    '''\n",
    "    classes = clf.classes_\n",
    "    feat_importance = {}\n",
    "    for i in range(len(classes)):\n",
    "        importances = clf.estimators_[i].feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        feat_importance[classes[i]] = pd.DataFrame(X.columns, importances)    \n",
    "        feat_importance[classes[i]].reset_index(inplace=True)\n",
    "        feat_importance[classes[i]].columns = ['Importance','Feature']\n",
    "        feat_importance[classes[i]].sort_values(by='Importance', axis=0, ascending=True, inplace=True)\n",
    "        feat_importance[classes[i]].set_index('Feature',inplace=True)\n",
    "    return feat_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = 20\n",
    "sns.set_context(rc={\"figure.figsize\": (8, 6)})       \n",
    "importances = {}\n",
    "importances = get_feature_importances(model,X)\n",
    "for i in model.classes_[1:]:\n",
    "    print 'Top ' + str(n) + ' Features for ' + i\n",
    "    importances[i].index.name=None \n",
    "    importances[i].sort_values(by='Importance',ascending=False).head(n).plot(kind='barh',title=i.title() + ' Top ' + str(n) + ' Features',legend=False, color='lightblue')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "annotations['Actual'] = y\n",
    "annotations['Predicted'] = y_pred\n",
    "annotations['TextLength'] = annotations['CleanText'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missed = len(annotations[annotations['Predicted'] != annotations['Actual']])\n",
    "hit = len(annotations) - missed\n",
    "print 'Hit: ' + str(hit)\n",
    "print 'Missed: ' + str(missed)\n",
    "print 'Accuracy: ' + str(round(hit / float(len(annotations)),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "false_positives = annotations[(annotations['Predicted'] != annotations['Actual']) \n",
    "                              & (annotations['Predicted'] != 'other')]\n",
    "false_negatives = annotations[(annotations['Predicted'] != annotations['Actual']) \n",
    "                              & (annotations['Predicted'] == 'other')]\n",
    "true_positives = annotations[(annotations['Predicted'] == annotations['Actual']) \n",
    "                              & (annotations['Predicted'] != 'other')]\n",
    "print 'False Positives: ' + str(len(false_positives))\n",
    "print 'False Negatives: ' + str(len(false_negatives))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
