{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Omidyar Extractives Project 1\n",
    "## Extract Contract Text (Notebook 7 of 8)\n",
    "### Hash-based partitition function for segmenting documents prior to clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lib.py_rabin import rabin_partition, example_rabin_partition\n",
    "import re\n",
    "import time\n",
    "import subprocess\n",
    "import itertools\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import scipy.spatial.distance as ssd\n",
    "import scipy.cluster.hierarchy as hcluster\n",
    "import pandas as pd\n",
    "import collections as cl\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (20.0, 10.0)\n",
    "from IPython.core.debugger import Tracer\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove other characters\n",
    "char_to_remove = set(['.',',',';',':','-','_','[',']','&','`','@','*','^','|','~',';',':','\\'','\\\"',\">\",\"<\"]) \n",
    "def longstr_clean(longstr):\n",
    "    longstr = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\xff]', '', longstr) # remove all non-printable characters\n",
    "    longstr = re.compile(r'<.*?>').sub('', longstr) # strip html markup, e.g. <br>,<div>, etc...\n",
    "    longstr = longstr.replace(\"&nbsp\",'').replace(\"&lt;\",'').replace(\"&gt;\",'').replace(\"\\\\\",\"\") \n",
    "    longstr = ''.join(i for i in longstr if ord(i)<128 and i not in char_to_remove)\n",
    "    longstr = longstr.replace('\\r','').replace('\\n','').replace('\\t','')\n",
    "    longstr = \" \".join(longstr.split()).lower() # remove whitespace\n",
    "    return longstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load documents: 0.765012025833 seconds\n",
      "Time to clean document text: 146.658810854 seconds\n",
      "\n",
      "Total documents: 1496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index([u'ocid', u'category', u'contract_name', u'contract_identifier',\n",
       "       u'language', u'country_name', u'resource', u'contract_type',\n",
       "       u'signature_date', u'document_type', u'government_entity',\n",
       "       u'government_identifier', u'company_name', u'company_address',\n",
       "       u'jurisdiction_of_incorporation', u'registration_agency',\n",
       "       u'company_number', u'corporate_grouping', u'participation_share',\n",
       "       u'open_corporates_link', u'incorporation_date', u'operator',\n",
       "       u'project_title', u'project_identifier', u'license_name',\n",
       "       u'license_identifier', u'source_url', u'disclosure_mode',\n",
       "       u'retrieval_date', u'pdf_url', u'deal_number', u'contract_note',\n",
       "       u'matrix_page', u'annotation_category', u'annotation_text',\n",
       "       u'contract_text', u'contract_text_clean'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "#df = pd.read_pickle('contract_data/openland_contracts_with_text.pkl') # ~200 \n",
    "df = pd.read_pickle('contract_data/resource_contracts_with_text.pkl') # ~1500\n",
    "df.columns = [k.lower().replace(\" \",\"_\") for k in df.columns]\n",
    "print('Time to load documents: %s seconds' % str(time.time()-start) )\n",
    "\n",
    "\n",
    "# ## uncomment for training\n",
    "# df = df.iloc[:100]\n",
    "# ##\n",
    "\n",
    "df[\"contract_text_clean\"] = df.contract_text.apply(longstr_clean) \n",
    "print('Time to clean document text: %s seconds' % str(time.time()-start) )\n",
    "\n",
    "print('\\nTotal documents: %d' % len(df))\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rabin_graph(seriesDocs, paramRabin, flag=\"char\"):\n",
    "    \"\"\"\n",
    "    Cpu and memory optimized optimized graph construction\n",
    "    \n",
    "    :param serisDf: pandas series documents    \n",
    "    :param indexDf: pandas series index (default: range(len(series(df))))\n",
    "    :param paramRabin: \n",
    "    :param flag: similarity metric (default: fraction of text in common)\n",
    "    \n",
    "    :return G: NetworkX undirected graph (nodes=docs, edges=doc similarity)   \n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "fpraw = '_docRabinChunks'\n",
    "fpsorted = '_docRabinChunks_sorted'\n",
    "paramRabin = {\"avgchunk\":16, \"minchunk\":8, \"maxchunk\":32, \"windowsize\":8, \"windowslide\":1} \n",
    "flag=\"char\"\n",
    "\n",
    "# ----------------------------------\n",
    "# CREATE GRAPH NODES AND BREAK UP DOCUMENTS INTO RABIN CHUNKS\n",
    "# 1. identify chunks via 'rabin fingerprint', and w\n",
    "# 2. break up file marker-2-marker and sort by docFingerprint\n",
    "start = time.time()\n",
    "G = nx.Graph() \n",
    "with open(fpraw,'w') as fp:\n",
    "    for k in xrange(len(df.index)):\n",
    "        a = df.contract_text_clean.iloc[k]   # retrieve document text (must be clean already)\n",
    "        if len(a) <= 10:  continue\n",
    "        b = rabin_partition(a,**paramRabin)  # partition document via rabin fingerprint\n",
    "        c = set(b)\n",
    "\n",
    "        # add node to graph; \n",
    "        # also store document id, unique chunks, character length of unique chunks\n",
    "        G.add_node(k,\n",
    "                   ocid = df.ocid.iloc[k],\n",
    "                   n_chunk = len(c),\n",
    "                   n_char  = sum([len(i) for i in c]))\n",
    "        fp.writelines([str(i)+'|'+str(k)+'\\n' for i in c])\n",
    "print('Rabin chunks identified: %s seconds' % str(time.time()-start) )\n",
    "\n",
    "# ----------------------------------\n",
    "# in-memory pre-sort for all document chunks for speed up many-many comparison (O(n^2))\n",
    "cmd = 'sort -k1 -S2G ' + fpraw + ' > ' + fpsorted\n",
    "subprocess.call(cmd,shell=True)\n",
    "print('Rabin chunks sorted: %s seconds' % str(time.time()-start) )\n",
    "\n",
    "# ----------------------------------\n",
    "# UPDATE GRAPH WITH EDGE WEIGHTS = node similarity\n",
    "with open(fpsorted,'r') as fp:\n",
    "    # read line by line\n",
    "    for ix,line in enumerate(fp):\n",
    "        chunk,k = line.strip('\\n').split('|')\n",
    "        if ix==0: tmp=[]; prev_chunk = chunk\n",
    "\n",
    "        # collect docs that share the same chunk (recall: chunks are sorted)\n",
    "        if len(prev_chunk) == len(chunk) and prev_chunk == chunk:\n",
    "            tmp.extend([int(k)])\n",
    "\n",
    "        # complete edge comparison, i.e.common chunks between document pairs    \n",
    "        else:\n",
    "            if len(tmp)>1:\n",
    "                lenchunk = float(len(prev_chunk))\n",
    "                for (m,n) in itertools.combinations(tmp,2):\n",
    "                    if not G.has_edge(m,n): \n",
    "                        G.add_edge(m, n, n_chunk_matched=0., n_char_matched=0.)\n",
    "                    G[m][n]['n_chunk_matched'] += 1. \n",
    "                    G[m][n]['n_char_matched']  += lenchunk\n",
    "            tmp=[int(k)]\n",
    "        prev_chunk = chunk \n",
    "\n",
    "    # compute weights for all edges based on jaccard distance = len(shared text) / len(shorter document) \n",
    "    for m,n in G.edges_iter():\n",
    "        if flag == \"char\":\n",
    "            G[m][n][\"weight\"] = G[m][n][\"n_char_matched\"] / min(G.node[m]['n_char'],G.node[n]['n_char']) \n",
    "        elif flag == \"chunk\":\n",
    "            G[m][n][\"weight\"] = G[m][n][\"n_chunk_matched\"] / min(G.node[m]['n_chunk'],G.node[n]['n_chunk']) \n",
    "        else:\n",
    "            assert True # No such distance metric is available\n",
    "\n",
    "# clean up tmp files \n",
    "cmd = 'rm '+fpraw+' && '+'rm '+fpsorted\n",
    "subprocess.call(cmd,shell=True)\n",
    "print('Documents compared for similarity: %s seconds'%str(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def identifyIsolates(G):\n",
    "    \"\"\"Identify isolated nodes, i.e. nodes not connected to any other\"\"\"\n",
    "    iso = nx.isolates(G)\n",
    "    return iso if len(iso)>0 else None \n",
    "def clipEdges(G,minWeight=0.01):\n",
    "    \"\"\"Clip weakly connected edges\"\"\"\n",
    "    if minWeight:\n",
    "        print('\\nClipped edges at Applied min similarity threshold: %f' % minWeight )\n",
    "        H = G.copy()\n",
    "        H.remove_edges_from([(u,v) for (u,v,d) in H.edges(data=True) if d['weight'] < minWeight])\n",
    "    return H if minWeight else G  \n",
    "\n",
    "# clip weak edges\n",
    "G = clipEdges(G,minWeight=0.01)\n",
    "\n",
    "# remove one-off documents (those not connected to any other)\n",
    "oneoffs  = identifyIsolates(G)        # identify isolated docs\n",
    "if oneoffs:\n",
    "    Goneoffs = nx.subgraph(G,oneoffs) # keep info in a separate graph\n",
    "    G.remove_nodes_from(oneoffs)      # remove oneoffs from the main network\n",
    "    print(\"\\nNumber of 'oneoff' documents: %d\"%len(oneoffs))\n",
    "\n",
    "# Graph summary\n",
    "print('\\nGraph/network summary:')\n",
    "print nx.info(G)\n",
    "print('Average density: %f' % nx.density(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classic ward hierarchical clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## classic *ward* hierarchical clustering \n",
    "k = 0\n",
    "N = len(G.nodes())\n",
    "dMatrix = np.zeros([N*(N-1)/2,1])\n",
    "for m in G.nodes():\n",
    "    for n in G.nodes():\n",
    "        if n>m:\n",
    "            #if G.has_edge(m,n) and G[m][n][\"weight\"] > 0.95: print m,n,G[m][n][\"weight\"]             \n",
    "            dMatrix[k,0] = 1. - G[m][n][\"weight\"] if G.has_edge(m,n) else 1.\n",
    "            k+=1\n",
    "dMatrix = dMatrix.flatten()\n",
    "linkage = hcluster.ward(dMatrix)\n",
    "dendro  = hcluster.dendrogram(linkage,orientation='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Louvain community detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def merge_two_dicts(x, y):\n",
    "    \"\"\"Given two dicts, merge them into a new dict as a shallow copy.\"\"\"\n",
    "    z=x.copy()\n",
    "    return z.update(y)\n",
    "\n",
    "def louvain(G,labelBase=None):\n",
    "    \"\"\"\n",
    "    Hierchical clustering algorithm. Best-in-class winner 2010 \n",
    "    ref: V.D. Blondel, J.-L. Guillaume, R. Lambiotte, E. Lefebvre. \n",
    "         \"Fast unfolding of communities in large networks.\" J. Stat. Mech., 2008: 1008\n",
    "    \n",
    "    :param G: NetworkX undirected graph contaning nodes, edges, and edge weights\n",
    "    :param labelBase: used to convert default 0,1,2... to labelBase.0, labelBase.1, etc... \n",
    "    :return D: ordered dictionary containing cluster labels and G.nodes() belonging to each cluster \n",
    "    \"\"\"\n",
    "    import community as cm\n",
    "    D=cl.OrderedDict()\n",
    "    p=cm.best_partition(G)\n",
    "    for label in set(p.values()):\n",
    "        newlabel = str(labelBase)+\".\"+str(int(label)) if labelBase else str(int(label))\n",
    "        D[newlabel] = sorted([k for k in p.keys() if p[k] == label])\n",
    "    return D\n",
    "\n",
    "def recursive_louvain(G,maxLevel=10):\n",
    "    \"\"\"\n",
    "    Recursive wrapper for lovain clustering algorithm\n",
    "    \n",
    "    :param G: symmetric networkx graph containg nodes, edges, and edge weights\n",
    "    :param maxLevel: max recursive depth  \n",
    "    :return D: ordered dictionary containing cluster labels and G.nodes() belonging to each cluster \n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    D = cl.OrderedDict()\n",
    "    for k in xrange(maxLevel):\n",
    "        prev_level = \"Level_%02d\"%(k-1)\n",
    "        next_level = \"Level_%02d\"%(k)\n",
    "        d={}    \n",
    "        # level zero\n",
    "        if k==0: \n",
    "            D[next_level] = louvain(G)\n",
    "        \n",
    "        # level 1+\n",
    "        else: \n",
    "            # loop through all clusters at previous level\n",
    "            for key,val in D[prev_level].iteritems():\n",
    "                d2 = louvain(G.subgraph(val),key)\n",
    "                if len(d2.keys())==1: \n",
    "                    d = merge_two_dicts(d,{key:val})\n",
    "                else:\n",
    "                    try: \n",
    "                        d = merge_two_dicts(d,d2)\n",
    "                    except:\n",
    "                        Tracer()()\n",
    "            \n",
    "            # check if max depth is reached / further partition is possible\n",
    "            if len(d.keys()) > len(D[prev_level].keys()) and k<maxLevel:\n",
    "                D[next_level] = d.copy()\n",
    "            else:\n",
    "                print('Max cluster depth: %s'%prev_level)\n",
    "                print('Time to complete recursive louvain: %s seconds'%str(time.time()-start))\n",
    "                break\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D = recursive_louvain(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare for csv output\n",
    "df2 = df[[\"ocid\"]]\n",
    "for level in D.keys():\n",
    "    df2[level] = None\n",
    "    for key,val in D[level].iteritems():\n",
    "        df2.loc[val,level] = key\n",
    "df2.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[[\"ocid\",\"template\"]].to_csv(\"resource_contracts_with_text_louvain.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### reference code --------------------------- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "longstr1 = df.iloc[5].contract_text_clean\n",
    "longstr2 = df.iloc[14].contract_text_clean\n",
    "fuzz.ratio(longstr1, longstr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "minchunk,avgchunk,maxchunk=32,64,128\n",
    "a = set(rabin_partition(longstr1,avgchunk,minchunk,maxchunk,32))\n",
    "b = set(rabin_partition(longstr2,avgchunk,minchunk,maxchunk,32))\n",
    "\n",
    "print\n",
    "print \"EXAMPLE USE CASE: DOCUMENT SIMILARITY\"\n",
    "print\n",
    "\n",
    "print\n",
    "print \"Percent similarity between document 1 and 2: \\n%0.2f%%\"%( 100.*len(a&b)/float(min(len(a),len(b))) )\n",
    "\n",
    "print\n",
    "print \"Common rabin chunks between documents 1 and 2:\"\n",
    "print [k for k in rabin_partition(longstr1) if k in set(rabin_partition(longstr2))]\n",
    "\n",
    "print\n",
    "print \"Non-common rabin chunks between document 1 and 2:\"\n",
    "print [k for k in rabin_partition(longstr1) if k not in set(rabin_partition(longstr2))]\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clustering based on connected components\n",
    "def find_cluster_cutoff(G, cutoff = 0.10, minCluster=0):\n",
    "    print('\\nSimilarity cutoff: %f' % cutoff )\n",
    "    H = G.copy()\n",
    "    H.remove_edges_from([(u,v) for (u,v,d) in H.edges(data=True) if d['weight'] < cutoff])\n",
    "    clusters = [sorted(i) for i in sorted(nx.connected_components(H),key=len,reverse=True) if len(i)>minCluster]\n",
    "    print_cluster_summary(G,clusters)\n",
    "    return clusters\n",
    "\n",
    "# summary of cluster results\n",
    "def print_cluster_summary(G,clusters):\n",
    "    print('\\nNumber of clusters identified: %d' % len(clusters))\n",
    "    print('Document coverage: %d%% (%d of %d)' % \\\n",
    "          (100 * sum([len(i) for i in clusters])/len(G.node), sum([len(i) for i in clusters]), len(G.node)))\n",
    "    print('\\nCluster sizes:')\n",
    "    print([len(i) for i in clusters])\n",
    "\n",
    "# clustering based on lovain method \n",
    "def find_cluster_louvain(G,minCluster=0):\n",
    "    partition = cm.best_partition(G)\n",
    "    clusters = []\n",
    "    for label in set(partition.values()):\n",
    "        clusters.append([i for i in partition.keys() if partition[i] == label])\n",
    "    clusters = [sorted(i) for i in sorted(clusters, key = len, reverse=True) if len(i)>minCluster]\n",
    "    print_cluster_summary(G,clusters)\n",
    "    return clusters\n",
    "\n",
    "# collect output\n",
    "def base_output(G,node,label):\n",
    "    doc_id     = G.node[node]['ocid']\n",
    "    doc_degree = G.degree(node) \n",
    "    return [doc_id,label,doc_degree]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
